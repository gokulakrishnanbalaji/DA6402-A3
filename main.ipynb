{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11817904,"sourceType":"datasetVersion","datasetId":7422957}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Added dataset manually, if you want to download it, you can access it from here https://github.com/google-research-datasets/dakshina. Here I have used Tamil language.","metadata":{}},{"cell_type":"markdown","source":"# Question 1","metadata":{}},{"cell_type":"code","source":"# Importing libraries\n\nimport torch\nimport torch.nn as nn","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Encoder RNN\n\nclass EncoderRNN(nn.Module):\n    def __init__(self, input_size, embed_size, hidden_size, cell_type='RNN', num_layers=1, dropout=0.0):\n        super(EncoderRNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.embedding = nn.Embedding(input_size, embed_size)\n        self.cell_type = cell_type.upper()\n        \n        rnn_class = {'RNN': nn.RNN, 'LSTM': nn.LSTM, 'GRU': nn.GRU}[self.cell_type]\n        self.rnn = rnn_class(\n            input_size=embed_size,\n            hidden_size=hidden_size,\n            num_layers=num_layers,\n            batch_first=True,\n            dropout=dropout if num_layers > 1 else 0.0\n        )\n\n    def forward(self, input_seq):\n        embedded = self.embedding(input_seq)\n        if self.cell_type == 'LSTM':\n            outputs, (hidden, cell) = self.rnn(embedded)\n            return outputs, (hidden, cell)\n        else:\n            outputs, hidden = self.rnn(embedded)\n            return outputs, hidden","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Decoder RNN\n\nclass DecoderRNN(nn.Module):\n    def __init__(self, output_size, embed_size, hidden_size, cell_type='RNN', num_layers=1, dropout=0.0):\n        super(DecoderRNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.embedding = nn.Embedding(output_size, embed_size)\n        self.dropout = nn.Dropout(dropout)\n        self.cell_type = cell_type.upper()\n        \n        rnn_class = {'RNN': nn.RNN, 'LSTM': nn.LSTM, 'GRU': nn.GRU}[self.cell_type]\n        self.rnn = rnn_class(\n            input_size=embed_size,\n            hidden_size=hidden_size,\n            num_layers=num_layers,\n            batch_first=True,\n            dropout=dropout if num_layers > 1 else 0.0\n        )\n        self.out = nn.Linear(hidden_size, output_size)\n\n    def forward(self, input_char, hidden):\n        embedded = self.embedding(input_char).unsqueeze(1)\n        embedded = self.dropout(embedded)\n        if self.cell_type == 'LSTM':\n            output, (hidden, cell) = self.rnn(embedded, hidden)\n            output = self.out(output.squeeze(1))\n            return output, (hidden, cell)\n        else:\n            output, hidden = self.rnn(embedded, hidden)\n            output = self.out(output.squeeze(1))\n            return output, hidden","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Seq2Seq model\n\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder):\n        super(Seq2Seq, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, source, target, teacher_forcing_ratio=0.5):\n        batch_size = source.size(0)\n        target_len = target.size(1)\n        outputs = torch.zeros(batch_size, target_len, self.decoder.out.out_features).to(source.device)\n\n        encoder_outputs, hidden = self.encoder(source)\n        \n        decoder_input = target[:, 0]\n        \n        # Handle differing encoder/decoder layers\n        if self.encoder.cell_type == 'LSTM':\n            hidden, cell = hidden\n            if self.encoder.num_layers != self.decoder.num_layers:\n                # Repeat or truncate hidden/cell states to match decoder layers\n                factor = self.decoder.num_layers // self.encoder.num_layers\n                if factor > 1:\n                    hidden = hidden.repeat(factor, 1, 1)\n                    cell = cell.repeat(factor, 1, 1)\n                else:\n                    hidden = hidden[-self.decoder.num_layers:]\n                    cell = cell[-self.decoder.num_layers:]\n            hidden = (hidden, cell)\n        else:\n            if self.encoder.num_layers != self.decoder.num_layers:\n                factor = self.decoder.num_layers // self.encoder.num_layers\n                if factor > 1:\n                    hidden = hidden.repeat(factor, 1, 1)\n                else:\n                    hidden = hidden[-self.decoder.num_layers:]\n        \n        for t in range(1, target_len):\n            output, hidden = self.decoder(decoder_input, hidden)\n            outputs[:, t, :] = output\n            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n            top1 = output.argmax(1)\n            decoder_input = target[:, t] if teacher_force else top1\n\n        return outputs","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Wrapper function to create model\n\ndef create_model(input_vocab_size, output_vocab_size, embed_size=256, hidden_size=512, \n                 cell_type='RNN', encoder_layers=1, decoder_layers=1, dropout=0.0):\n    encoder = EncoderRNN(input_vocab_size, embed_size, hidden_size, cell_type, encoder_layers, dropout)\n    decoder = DecoderRNN(output_vocab_size, embed_size, hidden_size, cell_type, decoder_layers, dropout)\n    model = Seq2Seq(encoder, decoder)\n    return model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Question 2","metadata":{}},{"cell_type":"markdown","source":"## Preparing dataset for training","metadata":{}},{"cell_type":"code","source":"# Importing Libraries\n\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch import optim\nfrom collections import Counter\nimport os","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Vocabulary class to handle character-to-index mapping\nclass Vocabulary:\n    def __init__(self):\n        self.char2idx = {'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3}\n        self.idx2char = {0: '<PAD>', 1: '<SOS>', 2: '<EOS>', 3: '<UNK>'}\n        self.size = 4\n\n    def add_sequence(self, sequence):\n        for char in sequence:\n            if char not in self.char2idx:\n                self.char2idx[char] = self.size\n                self.idx2char[self.size] = char\n                self.size += 1\n\n    def get_indices(self, sequence):\n        indices = [self.char2idx.get(char, self.char2idx['<UNK>']) for char in sequence]\n        return indices","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Custom Dataset class\n\nclass DakshinaDataset(Dataset):\n    def __init__(self, data, src_vocab, tgt_vocab):\n        self.data = data\n        self.src_vocab = src_vocab\n        self.tgt_vocab = tgt_vocab\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        src = self.data.iloc[idx, 1]  # English (Latin)\n        tgt = self.data.iloc[idx, 0]  # Tamil\n        src_indices = [self.src_vocab.char2idx['<SOS>']] + self.src_vocab.get_indices(src) + [self.src_vocab.char2idx['<EOS>']]\n        tgt_indices = [self.tgt_vocab.char2idx['<SOS>']] + self.tgt_vocab.get_indices(tgt) + [self.tgt_vocab.char2idx['<EOS>']]\n        return torch.tensor(src_indices, dtype=torch.long), torch.tensor(tgt_indices, dtype=torch.long)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to load and preprocess data\ndef load_dakshina_data(train_path, val_path, test_path):\n    # Read TSV files without headers\n    train_df = pd.read_csv(train_path, sep='\\t', header=None, usecols=[0, 1])\n    val_df = pd.read_csv(val_path, sep='\\t', header=None, usecols=[0, 1])\n    test_df = pd.read_csv(test_path, sep='\\t', header=None, usecols=[0, 1])\n\n    # Ensure strings\n    train_df[0] = train_df[0].astype(str)\n    train_df[1] = train_df[1].astype(str)\n    val_df[0] = val_df[0].astype(str)\n    val_df[1] = val_df[1].astype(str)\n    test_df[0] = test_df[0].astype(str)\n    test_df[1] = test_df[1].astype(str)\n\n    # Build vocabularies\n    src_vocab = Vocabulary()  # English (Latin)\n    tgt_vocab = Vocabulary()  # Tamil\n\n    # Add characters to vocab from training data\n    for _, row in train_df.iterrows():\n        src_vocab.add_sequence(row[1])\n        tgt_vocab.add_sequence(row[0])\n\n    # Create datasets\n    train_dataset = DakshinaDataset(train_df, src_vocab, tgt_vocab)\n    val_dataset = DakshinaDataset(val_df, src_vocab, tgt_vocab)\n    test_dataset = DakshinaDataset(test_df, src_vocab, tgt_vocab)\n\n    return train_dataset, val_dataset, test_dataset, src_vocab, tgt_vocab\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Collate function for DataLoader\ndef collate_fn(batch):\n    src_batch, tgt_batch = zip(*batch)\n    # Pad sequences\n    src_padded = torch.nn.utils.rnn.pad_sequence(src_batch, batch_first=True, padding_value=0)\n    tgt_padded = torch.nn.utils.rnn.pad_sequence(tgt_batch, batch_first=True, padding_value=0)\n    return src_padded, tgt_padded","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Wrapper function for easier access\n\ndef prepare_data_loaders(train_path, val_path, test_path, batch_size=32):\n    train_dataset, val_dataset, test_dataset, src_vocab, tgt_vocab = load_dakshina_data(train_path, val_path, test_path)\n    \n    train_loader = DataLoader(\n        train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, pin_memory=True\n    )\n    val_loader = DataLoader(\n        val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, pin_memory=True\n    )\n    test_loader = DataLoader(\n        test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, pin_memory=True\n    )\n    \n    return train_loader, val_loader, test_loader, src_vocab, tgt_vocab","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Paths to your local TSV files (update as needed)\ntrain_path = '/kaggle/input/dakshina-tamil/ta.translit.sampled.train.tsv'\nval_path = '/kaggle/input/dakshina-tamil/ta.translit.sampled.dev.tsv'\ntest_path = '/kaggle/input/dakshina-tamil/ta.translit.sampled.test.tsv'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create data loaders\ntrain_loader, val_loader, test_loader, src_vocab, tgt_vocab = prepare_data_loaders(train_path, val_path, test_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Print vocabulary sizes\nprint(f\"Source (English) vocabulary size: {src_vocab.size}\")\nprint(f\"Target (Tamil) vocabulary size: {tgt_vocab.size}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Setting up wandb","metadata":{}},{"cell_type":"code","source":"!pip install wandb -q","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import wandb","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"wandb\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"wandb.login(key=secret_value_0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Running wandb sweep","metadata":{}},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Beam search decoding\ndef beam_search_decode(model, src, max_len, beam_width, sos_idx, eos_idx):\n    model.eval()\n    src = src.to(device)\n    batch_size = src.size(0)\n    encoder_outputs, hidden = model.encoder(src)\n    \n    if model.encoder.cell_type == 'LSTM':\n        hidden = (hidden[0], hidden[1])\n    \n    # Initialize beam\n    beams = [(torch.tensor([sos_idx], device=device), hidden, 0.0)]  # (sequence, hidden, score)\n    completed = []\n    \n    for _ in range(max_len):\n        new_beams = []\n        for seq, hid, score in beams:\n            if seq[-1].item() == eos_idx:\n                completed.append((seq, score))\n                continue\n            output, new_hidden = model.decoder(seq[-1].unsqueeze(0), hid)\n            probs = torch.softmax(output, dim=-1)\n            top_probs, top_idx = probs.topk(beam_width)\n            \n            for i in range(beam_width):\n                new_seq = torch.cat([seq, top_idx[:, i]])\n                new_score = score - math.log(top_probs[:, i].item())\n                new_beams.append((new_seq, new_hidden, new_score))\n        \n        # Keep top beam_width beams\n        new_beams = sorted(new_beams, key=lambda x: x[2])[:beam_width]\n        beams = new_beams\n        \n        if len(completed) >= beam_width:\n            break\n    \n    # Return best sequence\n    completed = sorted(completed, key=lambda x: x[1])\n    if completed:\n        return completed[0][0]\n    return beams[0][0]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training and evaluation function\ndef train_and_evaluate():\n    wandb.init()\n    config = wandb.config\n    \n    # Create model with sweep parameters\n    model = create_model(\n        input_vocab_size=src_vocab.size,\n        output_vocab_size=tgt_vocab.size,\n        embed_size=config.embed_size,\n        hidden_size=config.hidden_size,\n        cell_type=config.cell_type,\n        encoder_layers=config.encoder_layers,\n        decoder_layers=config.decoder_layers,\n        dropout=config.dropout\n    ).to(device)\n    \n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    criterion = nn.CrossEntropyLoss(ignore_index=0)\n    \n    for epoch in range(10):\n        model.train()\n        train_loss = 0\n        for src, tgt in train_loader:\n            src, tgt = src.to(device), tgt.to(device)\n            optimizer.zero_grad()\n            output = model(src, tgt, teacher_forcing_ratio=0.5)\n            output = output[:, 1:].reshape(-1, output.size(-1))\n            tgt = tgt[:, 1:].reshape(-1)\n            loss = criterion(output, tgt)\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n        \n        model.eval()\n        val_loss = 0\n        with torch.no_grad():\n            for src, tgt in val_loader:\n                src, tgt = src.to(device), tgt.to(device)\n                output = model(src, tgt, teacher_forcing_ratio=0.0)\n                output = output[:, 1:].reshape(-1, output.size(-1))\n                tgt = tgt[:, 1:].reshape(-1)\n                loss = criterion(output, tgt)\n                val_loss += loss.item()\n        \n        correct = 0\n        total = 0\n        with torch.no_grad():\n            for src, tgt in val_loader:\n                src, tgt = src.to(device), tgt.to(device)\n                for i in range(src.size(0)):\n                    pred = beam_search_decode(\n                        model, src[i:i+1], max_len=50,\n                        beam_width=config.beam_width,\n                        sos_idx=tgt_vocab.char2idx['<SOS>'],\n                        eos_idx=tgt_vocab.char2idx['<EOS>']\n                    )\n                    pred_str = ''.join([tgt_vocab.idx2char[idx.item()] for idx in pred if idx.item() not in [0, 1, 2]])\n                    tgt_str = ''.join([tgt_vocab.idx2char[idx.item()] for idx in tgt[i, 1:] if idx.item() not in [0, 1, 2]])\n                    if pred_str == tgt_str:\n                        correct += 1\n                    total += 1\n                break\n        \n        wandb.log({\n            \"epoch\": epoch,\n            \"train_loss\": train_loss / len(train_loader),\n            \"val_loss\": val_loss / len(val_loader),\n            \"val_accuracy\": correct / total\n        })","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# WandB sweep configuration\nsweep_config = {\n    'method': 'bayes',\n    'metric': {\n        'name': 'val_accuracy',\n        'goal': 'maximize'\n    },\n    'parameters': {\n        'embed_size': {\n            'values': [16,32,64, 128, 256]\n        },\n        'hidden_size': {\n            'values': [16,32,128, 256, 512]\n        },\n        'encoder_layers': {\n            'values': [1, 2, 3]\n        },\n        'decoder_layers': {\n            'values': [1, 2, 3]\n        },\n        'cell_type': {\n            'values': ['RNN', 'GRU', 'LSTM']\n        },\n        'dropout': {\n            'values': [0.2, 0.3]\n        },\n        'beam_width': {\n            'values': [1, 3, 5]\n        }\n    }\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize and run sweep\nsweep_id = wandb.sweep(sweep_config, project=\"DL-A3\")\nwandb.agent(sweep_id, function=train_and_evaluate, count=20)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}