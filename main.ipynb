{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e349472",
   "metadata": {
    "papermill": {
     "duration": 0.004831,
     "end_time": "2025-05-15T07:02:08.103432",
     "exception": false,
     "start_time": "2025-05-15T07:02:08.098601",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Added dataset manually, if you want to download it, you can access it from here https://github.com/google-research-datasets/dakshina. Here I have used Tamil language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f816f4",
   "metadata": {
    "papermill": {
     "duration": 0.003701,
     "end_time": "2025-05-15T07:02:08.111280",
     "exception": false,
     "start_time": "2025-05-15T07:02:08.107579",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bb26912",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T07:02:08.119842Z",
     "iopub.status.busy": "2025-05-15T07:02:08.119611Z",
     "iopub.status.idle": "2025-05-15T07:02:12.414191Z",
     "shell.execute_reply": "2025-05-15T07:02:12.413635Z"
    },
    "papermill": {
     "duration": 4.300536,
     "end_time": "2025-05-15T07:02:12.415611",
     "exception": false,
     "start_time": "2025-05-15T07:02:08.115075",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3aeea4a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T07:02:12.424828Z",
     "iopub.status.busy": "2025-05-15T07:02:12.424537Z",
     "iopub.status.idle": "2025-05-15T07:02:12.430203Z",
     "shell.execute_reply": "2025-05-15T07:02:12.429560Z"
    },
    "papermill": {
     "duration": 0.011437,
     "end_time": "2025-05-15T07:02:12.431352",
     "exception": false,
     "start_time": "2025-05-15T07:02:12.419915",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Encoder RNN\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, embed_size, hidden_size, cell_type='RNN', num_layers=1, dropout=0.0):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(input_size, embed_size)\n",
    "        self.cell_type = cell_type.upper()\n",
    "        \n",
    "        rnn_class = {'RNN': nn.RNN, 'LSTM': nn.LSTM, 'GRU': nn.GRU}[self.cell_type]\n",
    "        self.rnn = rnn_class(\n",
    "            input_size=embed_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0.0\n",
    "        )\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        embedded = self.embedding(input_seq)\n",
    "        if self.cell_type == 'LSTM':\n",
    "            outputs, (hidden, cell) = self.rnn(embedded)\n",
    "            return outputs, (hidden, cell)\n",
    "        else:\n",
    "            outputs, hidden = self.rnn(embedded)\n",
    "            return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5612b93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T07:02:12.440012Z",
     "iopub.status.busy": "2025-05-15T07:02:12.439811Z",
     "iopub.status.idle": "2025-05-15T07:02:12.445374Z",
     "shell.execute_reply": "2025-05-15T07:02:12.444905Z"
    },
    "papermill": {
     "duration": 0.01131,
     "end_time": "2025-05-15T07:02:12.446602",
     "exception": false,
     "start_time": "2025-05-15T07:02:12.435292",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Decoder RNN\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, output_size, embed_size, hidden_size, cell_type='RNN', num_layers=1, dropout=0.0):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(output_size, embed_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.cell_type = cell_type.upper()\n",
    "        \n",
    "        rnn_class = {'RNN': nn.RNN, 'LSTM': nn.LSTM, 'GRU': nn.GRU}[self.cell_type]\n",
    "        self.rnn = rnn_class(\n",
    "            input_size=embed_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0.0\n",
    "        )\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input_char, hidden):\n",
    "        embedded = self.embedding(input_char).unsqueeze(1)\n",
    "        embedded = self.dropout(embedded)\n",
    "        if self.cell_type == 'LSTM':\n",
    "            output, (hidden, cell) = self.rnn(embedded, hidden)\n",
    "            output = self.out(output.squeeze(1))\n",
    "            return output, (hidden, cell)\n",
    "        else:\n",
    "            output, hidden = self.rnn(embedded, hidden)\n",
    "            output = self.out(output.squeeze(1))\n",
    "            return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc5d21ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T07:02:12.455169Z",
     "iopub.status.busy": "2025-05-15T07:02:12.454972Z",
     "iopub.status.idle": "2025-05-15T07:02:12.461940Z",
     "shell.execute_reply": "2025-05-15T07:02:12.461416Z"
    },
    "papermill": {
     "duration": 0.01254,
     "end_time": "2025-05-15T07:02:12.462959",
     "exception": false,
     "start_time": "2025-05-15T07:02:12.450419",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Seq2Seq model\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, source, target, teacher_forcing_ratio=0.5):\n",
    "        batch_size = source.size(0)\n",
    "        target_len = target.size(1)\n",
    "        outputs = torch.zeros(batch_size, target_len, self.decoder.out.out_features).to(source.device)\n",
    "\n",
    "        encoder_outputs, hidden = self.encoder(source)\n",
    "        \n",
    "        decoder_input = target[:, 0]\n",
    "        \n",
    "        # Handle differing encoder/decoder layers\n",
    "        if self.encoder.cell_type == 'LSTM':\n",
    "            hidden, cell = hidden\n",
    "            if self.encoder.num_layers != self.decoder.num_layers:\n",
    "                # Compute repeat factor or select layers\n",
    "                if self.decoder.num_layers > self.encoder.num_layers:\n",
    "                    factor = self.decoder.num_layers // self.encoder.num_layers + 1\n",
    "                    hidden = hidden.repeat(factor, 1, 1)[:self.decoder.num_layers]\n",
    "                    cell = cell.repeat(factor, 1, 1)[:self.decoder.num_layers]\n",
    "\n",
    "                else:\n",
    "                    hidden = hidden[-self.decoder.num_layers:]\n",
    "                    cell = cell[-self.decoder.num_layers:]\n",
    "            hidden = (hidden, cell)\n",
    "        else:\n",
    "            # For RNN and GRU\n",
    "            if self.encoder.num_layers != self.decoder.num_layers:\n",
    "                if self.decoder.num_layers > self.encoder.num_layers:\n",
    "                    factor = self.decoder.num_layers // self.encoder.num_layers + 1\n",
    "                    hidden = hidden.repeat(factor, 1, 1)[:self.decoder.num_layers]\n",
    "                else:\n",
    "                    hidden = hidden[-self.decoder.num_layers:]\n",
    "        \n",
    "        for t in range(1, target_len):\n",
    "            output, hidden = self.decoder(decoder_input, hidden)\n",
    "            outputs[:, t, :] = output\n",
    "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)\n",
    "            decoder_input = target[:, t] if teacher_force else top1\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "050d011f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T07:02:12.471480Z",
     "iopub.status.busy": "2025-05-15T07:02:12.471266Z",
     "iopub.status.idle": "2025-05-15T07:02:12.474983Z",
     "shell.execute_reply": "2025-05-15T07:02:12.474476Z"
    },
    "papermill": {
     "duration": 0.009197,
     "end_time": "2025-05-15T07:02:12.475975",
     "exception": false,
     "start_time": "2025-05-15T07:02:12.466778",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Wrapper function to create model\n",
    "\n",
    "def create_model(input_vocab_size, output_vocab_size, embed_size=256, hidden_size=512, \n",
    "                 cell_type='RNN', encoder_layers=1, decoder_layers=1, dropout=0.0):\n",
    "    encoder = EncoderRNN(input_vocab_size, embed_size, hidden_size, cell_type, encoder_layers, dropout)\n",
    "    decoder = DecoderRNN(output_vocab_size, embed_size, hidden_size, cell_type, decoder_layers, dropout)\n",
    "    model = Seq2Seq(encoder, decoder)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24da1ff6",
   "metadata": {
    "papermill": {
     "duration": 0.003774,
     "end_time": "2025-05-15T07:02:12.483608",
     "exception": false,
     "start_time": "2025-05-15T07:02:12.479834",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab8a21b",
   "metadata": {
    "papermill": {
     "duration": 0.00363,
     "end_time": "2025-05-15T07:02:12.491272",
     "exception": false,
     "start_time": "2025-05-15T07:02:12.487642",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Preparing dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "803fe0c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T07:02:12.499924Z",
     "iopub.status.busy": "2025-05-15T07:02:12.499752Z",
     "iopub.status.idle": "2025-05-15T07:02:13.857895Z",
     "shell.execute_reply": "2025-05-15T07:02:13.857332Z"
    },
    "papermill": {
     "duration": 1.363688,
     "end_time": "2025-05-15T07:02:13.859181",
     "exception": false,
     "start_time": "2025-05-15T07:02:12.495493",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim\n",
    "from collections import Counter\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b95116fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T07:02:13.867998Z",
     "iopub.status.busy": "2025-05-15T07:02:13.867719Z",
     "iopub.status.idle": "2025-05-15T07:02:13.872367Z",
     "shell.execute_reply": "2025-05-15T07:02:13.871868Z"
    },
    "papermill": {
     "duration": 0.009979,
     "end_time": "2025-05-15T07:02:13.873294",
     "exception": false,
     "start_time": "2025-05-15T07:02:13.863315",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Vocabulary class to handle character-to-index mapping\n",
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        self.char2idx = {'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3}\n",
    "        self.idx2char = {0: '<PAD>', 1: '<SOS>', 2: '<EOS>', 3: '<UNK>'}\n",
    "        self.size = 4\n",
    "\n",
    "    def add_sequence(self, sequence):\n",
    "        for char in sequence:\n",
    "            if char not in self.char2idx:\n",
    "                self.char2idx[char] = self.size\n",
    "                self.idx2char[self.size] = char\n",
    "                self.size += 1\n",
    "\n",
    "    def get_indices(self, sequence):\n",
    "        indices = [self.char2idx.get(char, self.char2idx['<UNK>']) for char in sequence]\n",
    "        return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb0dc8e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T07:02:13.881813Z",
     "iopub.status.busy": "2025-05-15T07:02:13.881617Z",
     "iopub.status.idle": "2025-05-15T07:02:13.886303Z",
     "shell.execute_reply": "2025-05-15T07:02:13.885825Z"
    },
    "papermill": {
     "duration": 0.009961,
     "end_time": "2025-05-15T07:02:13.887167",
     "exception": false,
     "start_time": "2025-05-15T07:02:13.877206",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Custom Dataset class\n",
    "\n",
    "class DakshinaDataset(Dataset):\n",
    "    def __init__(self, data, src_vocab, tgt_vocab):\n",
    "        self.data = data\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src = self.data.iloc[idx, 1]  # English (Latin)\n",
    "        tgt = self.data.iloc[idx, 0]  # Tamil\n",
    "        src_indices = [self.src_vocab.char2idx['<SOS>']] + self.src_vocab.get_indices(src) + [self.src_vocab.char2idx['<EOS>']]\n",
    "        tgt_indices = [self.tgt_vocab.char2idx['<SOS>']] + self.tgt_vocab.get_indices(tgt) + [self.tgt_vocab.char2idx['<EOS>']]\n",
    "        return torch.tensor(src_indices, dtype=torch.long), torch.tensor(tgt_indices, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "498f8c01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T07:02:13.895485Z",
     "iopub.status.busy": "2025-05-15T07:02:13.895265Z",
     "iopub.status.idle": "2025-05-15T07:02:13.900712Z",
     "shell.execute_reply": "2025-05-15T07:02:13.900052Z"
    },
    "papermill": {
     "duration": 0.01091,
     "end_time": "2025-05-15T07:02:13.901831",
     "exception": false,
     "start_time": "2025-05-15T07:02:13.890921",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to load and preprocess data\n",
    "def load_dakshina_data(train_path, val_path, test_path):\n",
    "    # Read TSV files without headers\n",
    "    train_df = pd.read_csv(train_path, sep='\\t', header=None, usecols=[0, 1])\n",
    "    val_df = pd.read_csv(val_path, sep='\\t', header=None, usecols=[0, 1])\n",
    "    test_df = pd.read_csv(test_path, sep='\\t', header=None, usecols=[0, 1])\n",
    "\n",
    "    # Ensure strings\n",
    "    train_df[0] = train_df[0].astype(str)\n",
    "    train_df[1] = train_df[1].astype(str)\n",
    "    val_df[0] = val_df[0].astype(str)\n",
    "    val_df[1] = val_df[1].astype(str)\n",
    "    test_df[0] = test_df[0].astype(str)\n",
    "    test_df[1] = test_df[1].astype(str)\n",
    "\n",
    "    # Build vocabularies\n",
    "    src_vocab = Vocabulary()  # English (Latin)\n",
    "    tgt_vocab = Vocabulary()  # Tamil\n",
    "\n",
    "    # Add characters to vocab from training data\n",
    "    for _, row in train_df.iterrows():\n",
    "        src_vocab.add_sequence(row[1])\n",
    "        tgt_vocab.add_sequence(row[0])\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = DakshinaDataset(train_df, src_vocab, tgt_vocab)\n",
    "    val_dataset = DakshinaDataset(val_df, src_vocab, tgt_vocab)\n",
    "    test_dataset = DakshinaDataset(test_df, src_vocab, tgt_vocab)\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset, src_vocab, tgt_vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8f9bf98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T07:02:13.910311Z",
     "iopub.status.busy": "2025-05-15T07:02:13.909848Z",
     "iopub.status.idle": "2025-05-15T07:02:13.913534Z",
     "shell.execute_reply": "2025-05-15T07:02:13.912852Z"
    },
    "papermill": {
     "duration": 0.008878,
     "end_time": "2025-05-15T07:02:13.914561",
     "exception": false,
     "start_time": "2025-05-15T07:02:13.905683",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Collate function for DataLoader\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = zip(*batch)\n",
    "    # Pad sequences\n",
    "    src_padded = torch.nn.utils.rnn.pad_sequence(src_batch, batch_first=True, padding_value=0)\n",
    "    tgt_padded = torch.nn.utils.rnn.pad_sequence(tgt_batch, batch_first=True, padding_value=0)\n",
    "    return src_padded, tgt_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e374078d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T07:02:13.922712Z",
     "iopub.status.busy": "2025-05-15T07:02:13.922520Z",
     "iopub.status.idle": "2025-05-15T07:02:13.926695Z",
     "shell.execute_reply": "2025-05-15T07:02:13.926066Z"
    },
    "papermill": {
     "duration": 0.009442,
     "end_time": "2025-05-15T07:02:13.927708",
     "exception": false,
     "start_time": "2025-05-15T07:02:13.918266",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Wrapper function for easier access\n",
    "\n",
    "def prepare_data_loaders(train_path, val_path, test_path, batch_size=32):\n",
    "    train_dataset, val_dataset, test_dataset, src_vocab, tgt_vocab = load_dakshina_data(train_path, val_path, test_path)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, pin_memory=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, pin_memory=True\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, pin_memory=True\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, src_vocab, tgt_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce4738fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T07:02:13.936090Z",
     "iopub.status.busy": "2025-05-15T07:02:13.935773Z",
     "iopub.status.idle": "2025-05-15T07:02:13.938962Z",
     "shell.execute_reply": "2025-05-15T07:02:13.938307Z"
    },
    "papermill": {
     "duration": 0.008412,
     "end_time": "2025-05-15T07:02:13.939923",
     "exception": false,
     "start_time": "2025-05-15T07:02:13.931511",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Paths to your local TSV files (update as needed)\n",
    "train_path = '/kaggle/input/dakshina-tamil/ta.translit.sampled.train.tsv'\n",
    "val_path = '/kaggle/input/dakshina-tamil/ta.translit.sampled.dev.tsv'\n",
    "test_path = '/kaggle/input/dakshina-tamil/ta.translit.sampled.test.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6374e0bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T07:02:13.948406Z",
     "iopub.status.busy": "2025-05-15T07:02:13.947856Z",
     "iopub.status.idle": "2025-05-15T07:02:16.545897Z",
     "shell.execute_reply": "2025-05-15T07:02:16.545077Z"
    },
    "papermill": {
     "duration": 2.603623,
     "end_time": "2025-05-15T07:02:16.547318",
     "exception": false,
     "start_time": "2025-05-15T07:02:13.943695",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_loader, val_loader, test_loader, src_vocab, tgt_vocab = prepare_data_loaders(train_path, val_path, test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a48381bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T07:02:16.557323Z",
     "iopub.status.busy": "2025-05-15T07:02:16.557099Z",
     "iopub.status.idle": "2025-05-15T07:02:16.561444Z",
     "shell.execute_reply": "2025-05-15T07:02:16.560738Z"
    },
    "papermill": {
     "duration": 0.010259,
     "end_time": "2025-05-15T07:02:16.562597",
     "exception": false,
     "start_time": "2025-05-15T07:02:16.552338",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source (English) vocabulary size: 30\n",
      "Target (Tamil) vocabulary size: 50\n"
     ]
    }
   ],
   "source": [
    "# Print vocabulary sizes\n",
    "print(f\"Source (English) vocabulary size: {src_vocab.size}\")\n",
    "print(f\"Target (Tamil) vocabulary size: {tgt_vocab.size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea115558",
   "metadata": {
    "papermill": {
     "duration": 0.004202,
     "end_time": "2025-05-15T07:02:16.571317",
     "exception": false,
     "start_time": "2025-05-15T07:02:16.567115",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Setting up wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a34095c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T07:02:16.580676Z",
     "iopub.status.busy": "2025-05-15T07:02:16.580432Z",
     "iopub.status.idle": "2025-05-15T07:02:20.349726Z",
     "shell.execute_reply": "2025-05-15T07:02:20.348676Z"
    },
    "papermill": {
     "duration": 3.775421,
     "end_time": "2025-05-15T07:02:20.351119",
     "exception": false,
     "start_time": "2025-05-15T07:02:16.575698",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install wandb -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0667165e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T07:02:20.360408Z",
     "iopub.status.busy": "2025-05-15T07:02:20.360156Z",
     "iopub.status.idle": "2025-05-15T07:02:22.974702Z",
     "shell.execute_reply": "2025-05-15T07:02:22.974100Z"
    },
    "papermill": {
     "duration": 2.620597,
     "end_time": "2025-05-15T07:02:22.975987",
     "exception": false,
     "start_time": "2025-05-15T07:02:20.355390",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da2bc62d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T07:02:22.985350Z",
     "iopub.status.busy": "2025-05-15T07:02:22.984790Z",
     "iopub.status.idle": "2025-05-15T07:02:23.202072Z",
     "shell.execute_reply": "2025-05-15T07:02:23.201552Z"
    },
    "papermill": {
     "duration": 0.222984,
     "end_time": "2025-05-15T07:02:23.203238",
     "exception": false,
     "start_time": "2025-05-15T07:02:22.980254",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "secret_value_0 = user_secrets.get_secret(\"wandb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2402eef8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T07:02:23.212171Z",
     "iopub.status.busy": "2025-05-15T07:02:23.211959Z",
     "iopub.status.idle": "2025-05-15T07:02:23.779495Z",
     "shell.execute_reply": "2025-05-15T07:02:23.778959Z"
    },
    "papermill": {
     "duration": 0.573217,
     "end_time": "2025-05-15T07:02:23.780570",
     "exception": false,
     "start_time": "2025-05-15T07:02:23.207353",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mda24m007\u001b[0m (\u001b[33mda24m007-iit-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key=secret_value_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176333c5",
   "metadata": {
    "papermill": {
     "duration": 0.004059,
     "end_time": "2025-05-15T07:02:23.789024",
     "exception": false,
     "start_time": "2025-05-15T07:02:23.784965",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Running wandb sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e3f1e407",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T07:02:23.798109Z",
     "iopub.status.busy": "2025-05-15T07:02:23.797899Z",
     "iopub.status.idle": "2025-05-15T07:02:23.858839Z",
     "shell.execute_reply": "2025-05-15T07:02:23.858144Z"
    },
    "papermill": {
     "duration": 0.066729,
     "end_time": "2025-05-15T07:02:23.859999",
     "exception": false,
     "start_time": "2025-05-15T07:02:23.793270",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33528a46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T07:02:23.870161Z",
     "iopub.status.busy": "2025-05-15T07:02:23.869952Z",
     "iopub.status.idle": "2025-05-15T07:02:23.878376Z",
     "shell.execute_reply": "2025-05-15T07:02:23.877659Z"
    },
    "papermill": {
     "duration": 0.014822,
     "end_time": "2025-05-15T07:02:23.879586",
     "exception": false,
     "start_time": "2025-05-15T07:02:23.864764",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Beam search decoding\n",
    "def beam_search_decode(model, src, max_len, beam_width, sos_idx, eos_idx):\n",
    "    model.eval()\n",
    "    src = src.to(device)\n",
    "    batch_size = src.size(0)\n",
    "    encoder_outputs, hidden = model.encoder(src)\n",
    "    \n",
    "    # Adjust hidden state to match decoder layers\n",
    "    if model.encoder.cell_type == 'LSTM':\n",
    "        hidden, cell = hidden\n",
    "        if model.encoder.num_layers != model.decoder.num_layers:\n",
    "            factor = model.decoder.num_layers // model.encoder.num_layers\n",
    "            if factor > 1:\n",
    "                hidden = hidden.repeat(factor, 1, 1)\n",
    "                cell = cell.repeat(factor, 1, 1)\n",
    "            else:\n",
    "                hidden = hidden[-model.decoder.num_layers:]\n",
    "                cell = cell[-model.decoder.num_layers:]\n",
    "        hidden = (hidden, cell)\n",
    "    else:\n",
    "        if model.encoder.num_layers != model.decoder.num_layers:\n",
    "            factor = model.decoder.num_layers // model.encoder.num_layers\n",
    "            if factor > 1:\n",
    "                hidden = hidden.repeat(factor, 1, 1)\n",
    "            else:\n",
    "                hidden = hidden[-model.decoder.num_layers:]\n",
    "    \n",
    "    # Initialize beam\n",
    "    beams = [(torch.tensor([sos_idx], device=device), hidden, 0.0)]  # (sequence, hidden, score)\n",
    "    completed = []\n",
    "    \n",
    "    for _ in range(max_len):\n",
    "        new_beams = []\n",
    "        for seq, hid, score in beams:\n",
    "            if seq[-1].item() == eos_idx:\n",
    "                completed.append((seq, score))\n",
    "                continue\n",
    "            output, new_hidden = model.decoder(seq[-1].unsqueeze(0), hid)\n",
    "            probs = torch.softmax(output, dim=-1)\n",
    "            top_probs, top_idx = probs.topk(beam_width)\n",
    "            \n",
    "            for i in range(beam_width):\n",
    "                new_seq = torch.cat([seq, top_idx[:, i]])\n",
    "                new_score = score - math.log(top_probs[:, i].item())\n",
    "                new_beams.append((new_seq, new_hidden, new_score))\n",
    "        \n",
    "        # Keep top beam_width beams\n",
    "        new_beams = sorted(new_beams, key=lambda x: x[2])[:beam_width]\n",
    "        beams = new_beams\n",
    "        \n",
    "        if len(completed) >= beam_width:\n",
    "            break\n",
    "    \n",
    "    # Return best sequence\n",
    "    completed = sorted(completed, key=lambda x: x[1])\n",
    "    if completed:\n",
    "        return completed[0][0]\n",
    "    return beams[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6cc23423",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T07:02:23.889491Z",
     "iopub.status.busy": "2025-05-15T07:02:23.889251Z",
     "iopub.status.idle": "2025-05-15T07:02:23.900850Z",
     "shell.execute_reply": "2025-05-15T07:02:23.900316Z"
    },
    "papermill": {
     "duration": 0.017875,
     "end_time": "2025-05-15T07:02:23.901913",
     "exception": false,
     "start_time": "2025-05-15T07:02:23.884038",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training and evaluation function\n",
    "def train_and_evaluate():\n",
    "    wandb.init()\n",
    "    config = wandb.config\n",
    "    \n",
    "    # Create model with sweep parameters\n",
    "    model = create_model(\n",
    "        input_vocab_size=src_vocab.size,\n",
    "        output_vocab_size=tgt_vocab.size,\n",
    "        embed_size=config.embed_size,\n",
    "        hidden_size=config.hidden_size,\n",
    "        cell_type=config.cell_type,\n",
    "        encoder_layers=config.encoder_layers,\n",
    "        decoder_layers=config.decoder_layers,\n",
    "        dropout=config.dropout\n",
    "    ).to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    \n",
    "    for epoch in range(10):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for src, tgt in train_loader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(src, tgt, teacher_forcing_ratio=0.5)\n",
    "            output = output[:, 1:].reshape(-1, output.size(-1))\n",
    "            tgt = tgt[:, 1:].reshape(-1)\n",
    "            loss = criterion(output, tgt)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Compute training accuracy on one batch\n",
    "        model.eval()\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        with torch.no_grad():\n",
    "            for src, tgt in train_loader:\n",
    "                src, tgt = src.to(device), tgt.to(device)\n",
    "                for i in range(src.size(0)):\n",
    "                    pred = beam_search_decode(\n",
    "                        model, src[i:i+1], max_len=50,\n",
    "                        beam_width=config.beam_width,\n",
    "                        sos_idx=tgt_vocab.char2idx['<SOS>'],\n",
    "                        eos_idx=tgt_vocab.char2idx['<EOS>']\n",
    "                    )\n",
    "                    pred_str = ''.join([tgt_vocab.idx2char[idx.item()] for idx in pred if idx.item() not in [0, 1, 2]])\n",
    "                    tgt_str = ''.join([tgt_vocab.idx2char[idx.item()] for idx in tgt[i, 1:] if idx.item() not in [0, 1, 2]])\n",
    "                    if pred_str == tgt_str:\n",
    "                        train_correct += 1\n",
    "                    train_total += 1\n",
    "                break\n",
    "        \n",
    "        # Compute validation loss and accuracy\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for src, tgt in val_loader:\n",
    "                src, tgt = src.to(device), tgt.to(device)\n",
    "                output = model(src, tgt, teacher_forcing_ratio=0.0)\n",
    "                output = output[:, 1:].reshape(-1, output.size(-1))\n",
    "                tgt = tgt[:, 1:].reshape(-1)\n",
    "                loss = criterion(output, tgt)\n",
    "                val_loss += loss.item()\n",
    "            \n",
    "            # Validation accuracy on one batch\n",
    "            for src, tgt in val_loader:\n",
    "                src, tgt = src.to(device), tgt.to(device)\n",
    "                for i in range(src.size(0)):\n",
    "                    pred = beam_search_decode(\n",
    "                        model, src[i:i+1], max_len=50,\n",
    "                        beam_width=config.beam_width,\n",
    "                        sos_idx=tgt_vocab.char2idx['<SOS>'],\n",
    "                        eos_idx=tgt_vocab.char2idx['<EOS>']\n",
    "                    )\n",
    "                    pred_str = ''.join([tgt_vocab.idx2char[idx.item()] for idx in pred if idx.item() not in [0, 1, 2]])\n",
    "                    tgt_str = ''.join([tgt_vocab.idx2char[idx.item()] for idx in tgt[i, 1:] if idx.item() not in [0, 1, 2]])\n",
    "                    if pred_str == tgt_str:\n",
    "                        val_correct += 1\n",
    "                    val_total += 1\n",
    "                break\n",
    "        \n",
    "        # Log metrics to WandB\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": train_loss / len(train_loader),\n",
    "            \"val_loss\": val_loss / len(val_loader),\n",
    "            \"train_accuracy\": train_correct / train_total,\n",
    "            \"val_accuracy\": val_correct / val_total\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5a33719a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T07:02:23.911678Z",
     "iopub.status.busy": "2025-05-15T07:02:23.911410Z",
     "iopub.status.idle": "2025-05-15T07:02:23.915711Z",
     "shell.execute_reply": "2025-05-15T07:02:23.915216Z"
    },
    "papermill": {
     "duration": 0.010277,
     "end_time": "2025-05-15T07:02:23.916787",
     "exception": false,
     "start_time": "2025-05-15T07:02:23.906510",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# WandB sweep configuration\n",
    "sweep_config = {\n",
    "    'method': 'bayes',\n",
    "    'metric': {\n",
    "        'name': 'val_accuracy',\n",
    "        'goal': 'maximize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'embed_size': {\n",
    "            'values': [16,32,64, 128, 256]\n",
    "        },\n",
    "        'hidden_size': {\n",
    "            'values': [16,32,128, 256, 512]\n",
    "        },\n",
    "        'encoder_layers': {\n",
    "            'values': [1, 2, 3]\n",
    "        },\n",
    "        'decoder_layers': {\n",
    "            'values': [1, 2, 3]\n",
    "        },\n",
    "        'cell_type': {\n",
    "            'values': ['RNN', 'GRU', 'LSTM']\n",
    "        },\n",
    "        'dropout': {\n",
    "            'values': [0.2, 0.3]\n",
    "        },\n",
    "        'beam_width': {\n",
    "            'values': [1, 3, 5]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1eac0394",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T07:02:23.926342Z",
     "iopub.status.busy": "2025-05-15T07:02:23.926134Z",
     "iopub.status.idle": "2025-05-15T11:16:51.059606Z",
     "shell.execute_reply": "2025-05-15T11:16:51.058938Z"
    },
    "papermill": {
     "duration": 15267.15659,
     "end_time": "2025-05-15T11:16:51.077704",
     "exception": false,
     "start_time": "2025-05-15T07:02:23.921114",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 6krpo8ms\n",
      "Sweep URL: https://wandb.ai/da24m007-iit-madras/DL-A3/sweeps/6krpo8ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: efutol5l with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_width: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_layers: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_layers: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250515_070224-efutol5l\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mswift-sweep-1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3/sweeps/6krpo8ms\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3/runs/efutol5l\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▃▃▄▅▆▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss █▄▃▂▂▂▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss █▆▂▂▂▃▂▃▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 2.36622\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 2.73927\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mswift-sweep-1\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3/runs/efutol5l\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250515_070224-efutol5l/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: clfwa4k8 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_width: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250515_071248-clfwa4k8\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mgrateful-sweep-2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3/sweeps/6krpo8ms\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3/runs/clfwa4k8\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▃▃▄▅▆▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss █▅▄▃▃▂▂▂▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss █▆▅▄▃▂▂▂▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 1.93789\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 2.29941\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mgrateful-sweep-2\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3/runs/clfwa4k8\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250515_071248-clfwa4k8/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: o76e60si with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_layers: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250515_072221-o76e60si\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mbumbling-sweep-3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3/sweeps/6krpo8ms\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3/runs/o76e60si\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▃▃▄▅▆▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▃▆▆▆▇▆▇█▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss █▄▃▂▂▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▁▁▃▆▇▆▆█▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss █▄▂▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.71875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0.31319\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 0.82286\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mbumbling-sweep-3\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3/runs/o76e60si\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250515_072221-o76e60si/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: zva5wzdu with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250515_073612-zva5wzdu\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msage-sweep-4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3/sweeps/6krpo8ms\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3/runs/zva5wzdu\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▃▃▄▅▆▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▁▁▁▁▁█▁▁▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss █▆▅▄▃▃▂▂▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss █▇▆▄▄▃▂▂▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.03125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 1.73408\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 2.08032\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33msage-sweep-4\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3/runs/zva5wzdu\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250515_073612-zva5wzdu/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: pyhdjeyn with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_layers: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250515_074600-pyhdjeyn\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mgrateful-sweep-5\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3/sweeps/6krpo8ms\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3/runs/pyhdjeyn\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▃▃▄▅▆▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▄▄▇▆▆█▇▆▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss █▄▃▂▂▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▁▂▁▅▆▅▇▆█▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss █▃▂▂▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0.31568\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 0.91695\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mgrateful-sweep-5\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3/runs/pyhdjeyn\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250515_074600-pyhdjeyn/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: svrma61f with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_layers: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250515_080001-svrma61f\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mnorthern-sweep-6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3/sweeps/6krpo8ms\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3/runs/svrma61f\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▃▃▄▅▆▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▄▅▆▇▇██▅▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss █▃▂▂▂▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▁▄▅▇▅▅▇▇█▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss █▂▁▂▁▁▂▃▃▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.84375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0.14658\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.53125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 0.98114\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mnorthern-sweep-6\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3/runs/svrma61f\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250515_080001-svrma61f/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 81illvkp with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_layers: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250515_081503-81illvkp\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33molive-sweep-7\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3/sweeps/6krpo8ms\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3/runs/81illvkp\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▃▃▄▅▆▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▅▅█▆▅▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss █▃▂▂▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▁▃▅▁▅▃▆█▅▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss ▂▁▁▃▃▅▆▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.9375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0.09808\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.34375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 1.05703\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33molive-sweep-7\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3/runs/81illvkp\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250515_081503-81illvkp/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: lg4xucjp with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_layers: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250515_083026-lg4xucjp\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mclean-sweep-8\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3/sweeps/6krpo8ms\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3/runs/lg4xucjp\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▃▃▄▅▆▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▅▇▅▇▄█▆▇▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss █▂▂▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▃██▃▃▅▂▂▃▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss ▁▂▃▃▅▆▆▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.6875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0.28044\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.3125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 1.03042\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mclean-sweep-8\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3/runs/lg4xucjp\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250515_083026-lg4xucjp/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: msdk6k9g with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_width: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250515_084439-msdk6k9g\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mpolished-sweep-9\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3/sweeps/6krpo8ms\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3/runs/msdk6k9g\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▃▃▄▅▆▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▁▇█▅█▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss █▃▂▂▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▁▆▅▅█▅▄▅▃▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss ▆▂▁▂▂▅▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.9375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0.09417\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.34375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 1.07029\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mpolished-sweep-9\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3/runs/msdk6k9g\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250515_084439-msdk6k9g/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 325ijxf5 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_layers: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250515_085744-325ijxf5\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mrural-sweep-10\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3/sweeps/6krpo8ms\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3/runs/325ijxf5\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mrural-sweep-10\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3/runs/325ijxf5\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250515_085744-325ijxf5/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 325ijxf5 errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_19/726176173.py\", line 43, in train_and_evaluate\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     pred = beam_search_decode(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_19/3445422041.py\", line 38, in beam_search_decode\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output, new_hidden = model.decoder(seq[-1].unsqueeze(0), hid)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_19/308851571.py\", line 26, in forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output, (hidden, cell) = self.rnn(embedded, hidden)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py\", line 1120, in forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.check_forward_args(input, hx, batch_sizes)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py\", line 1003, in check_forward_args\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.check_hidden_size(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py\", line 347, in check_hidden_size\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     raise RuntimeError(msg.format(expected_hidden_size, list(hx.size())))\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m RuntimeError: Expected hidden[0] size (3, 1, 256), got [2, 1, 256]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: pmgqawyv with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_layers: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250515_085921-pmgqawyv\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msweepy-sweep-11\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3/sweeps/6krpo8ms\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3/runs/pmgqawyv\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33msweepy-sweep-11\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3/runs/pmgqawyv\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250515_085921-pmgqawyv/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run pmgqawyv errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_19/726176173.py\", line 43, in train_and_evaluate\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     pred = beam_search_decode(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_19/3445422041.py\", line 38, in beam_search_decode\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output, new_hidden = model.decoder(seq[-1].unsqueeze(0), hid)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_19/308851571.py\", line 26, in forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output, (hidden, cell) = self.rnn(embedded, hidden)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py\", line 1120, in forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.check_forward_args(input, hx, batch_sizes)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py\", line 1003, in check_forward_args\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.check_hidden_size(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py\", line 347, in check_hidden_size\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     raise RuntimeError(msg.format(expected_hidden_size, list(hx.size())))\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m RuntimeError: Expected hidden[0] size (3, 1, 512), got [2, 1, 512]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: vqi2q2tf with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_layers: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250515_090058-vqi2q2tf\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mstellar-sweep-12\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3/sweeps/6krpo8ms\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3/runs/vqi2q2tf\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▃▃▄▅▆▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▄▆▄█▆▇▆▇▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss █▃▂▂▂▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▁▃▅▅█▇▇▅▅▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss █▂▁▁▁▃▂▂▃▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.78125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0.14639\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.28125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 0.96541\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mstellar-sweep-12\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3/runs/vqi2q2tf\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250515_090058-vqi2q2tf/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: jm17utx6 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_layers: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250515_091630-jm17utx6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mavid-sweep-13\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3/sweeps/6krpo8ms\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3/runs/jm17utx6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▃▃▄▅▆▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▁▁▇▄▆▇▄▃█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss █▂▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▆▇▅▆█▄▆▃▄▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss ▂▁▂▃▅▄▄███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0.28728\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.21875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 1.00599\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mavid-sweep-13\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3/runs/jm17utx6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250515_091630-jm17utx6/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: kh3b1e7k with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_layers: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250515_093043-kh3b1e7k\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mplayful-sweep-14\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3/sweeps/6krpo8ms\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3/runs/kh3b1e7k\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▃▃▄▅▆▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▄▂▆█▅▇▇▅▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss █▃▂▂▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▁▅▁▅▅█▃▅▅▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss ▃▁▂▃▅▅▅▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.90625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0.09188\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 1.09412\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mplayful-sweep-14\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3/runs/kh3b1e7k\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250515_093043-kh3b1e7k/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: e08s70lq with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_layers: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250515_094626-e08s70lq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdry-sweep-15\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3/sweeps/6krpo8ms\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3/runs/e08s70lq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▃▃▄▅▆▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▆▅▆▇▇▇█▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss █▃▂▂▂▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▁▅▆▅▅█▆█▅▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss █▂▂▁▁▂▂▃▃▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.84375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0.14029\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.40625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 1.00541\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mdry-sweep-15\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3/runs/e08s70lq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250515_094626-e08s70lq/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: d9vgt9j0 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_layers: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250515_100158-d9vgt9j0\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mstellar-sweep-16\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3/sweeps/6krpo8ms\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3/runs/d9vgt9j0\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▃▃▄▅▆▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▄▅▆▇█▇▆▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss █▃▂▂▂▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▂▁▂▅▅▅▃█▆▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss █▂▂▁▁▂▂▃▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.90625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0.15035\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.34375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 0.9749\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mstellar-sweep-16\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3/runs/d9vgt9j0\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250515_100158-d9vgt9j0/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ozg7xwod with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_width: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_layers: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250515_101722-ozg7xwod\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mrose-sweep-17\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3/sweeps/6krpo8ms\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3/runs/ozg7xwod\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▃▃▄▅▆▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▅▆▇▆▇▆▆█▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss █▃▂▂▂▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▂█▁▅▁▁▃▇▃▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss ▆▂▁▂▄▄▆▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0.12045\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.3125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 1.05086\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mrose-sweep-17\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3/runs/ozg7xwod\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250515_101722-ozg7xwod/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: amff4xek with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250515_103314-amff4xek\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdistinctive-sweep-18\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3/sweeps/6krpo8ms\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3/runs/amff4xek\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▃▃▄▅▆▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▁▄▅▅▇▅▆▅█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss █▃▂▂▂▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▁▃▄▆▃▄█▃▄▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss █▂▂▁▂▃▃▃▄▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.9375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0.1523\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.28125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 0.95767\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mdistinctive-sweep-18\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3/runs/amff4xek\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250515_103314-amff4xek/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: kc8t8w4r with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_layers: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250515_104616-kc8t8w4r\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mvaliant-sweep-19\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3/sweeps/6krpo8ms\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3/runs/kc8t8w4r\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▃▃▄▅▆▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▄▅▇▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss █▃▂▂▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▂▂▄▂▂▃▃▄█▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss ▃▁▁▃▅▅▆▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.90625\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0.09994\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.28125\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 1.09808\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mvaliant-sweep-19\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3/runs/kc8t8w4r\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250515_104616-kc8t8w4r/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: tqj7k9cv with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_width: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_layers: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250515_110144-tqj7k9cv\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mpolished-sweep-20\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3/sweeps/6krpo8ms\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3/runs/tqj7k9cv\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch ▁▂▃▃▄▅▆▆▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy ▁▄▆▅▆▇▇██▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss █▃▂▂▂▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy ▁▂▆▅▆▅▇▅█▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss █▂▁▁▁▂▂▂▂▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          epoch 9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_accuracy 0.84375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train_loss 0.15841\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_accuracy 0.375\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       val_loss 1.00211\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mpolished-sweep-20\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3/runs/tqj7k9cv\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/da24m007-iit-madras/DL-A3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250515_110144-tqj7k9cv/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Initialize and run sweep\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"DL-A3\")\n",
    "wandb.agent(sweep_id, function=train_and_evaluate, count=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a29971e",
   "metadata": {
    "papermill": {
     "duration": 0.018287,
     "end_time": "2025-05-15T11:16:51.114632",
     "exception": false,
     "start_time": "2025-05-15T11:16:51.096345",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7422957,
     "sourceId": 11817904,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 15291.190881,
   "end_time": "2025-05-15T11:16:55.227967",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-15T07:02:04.037086",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
