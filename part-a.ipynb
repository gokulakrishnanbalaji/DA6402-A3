{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11817904,"sourceType":"datasetVersion","datasetId":7422957}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Added dataset manually, if you want to download it, you can access it from here https://github.com/google-research-datasets/dakshina. Here I have used Tamil language.","metadata":{}},{"cell_type":"markdown","source":"# Question 1","metadata":{}},{"cell_type":"code","source":"# Importing libraries\n\nimport torch\nimport torch.nn as nn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T15:32:03.832205Z","iopub.execute_input":"2025-05-15T15:32:03.832365Z","iopub.status.idle":"2025-05-15T15:32:08.146301Z","shell.execute_reply.started":"2025-05-15T15:32:03.832349Z","shell.execute_reply":"2025-05-15T15:32:08.145507Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Encoder RNN\n\nclass EncoderRNN(nn.Module):\n    def __init__(self, input_size, embed_size, hidden_size, cell_type='RNN', num_layers=1, dropout=0.0):\n        super(EncoderRNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.embedding = nn.Embedding(input_size, embed_size)\n        self.cell_type = cell_type.upper()\n        \n        rnn_class = {'RNN': nn.RNN, 'LSTM': nn.LSTM, 'GRU': nn.GRU}[self.cell_type]\n        self.rnn = rnn_class(\n            input_size=embed_size,\n            hidden_size=hidden_size,\n            num_layers=num_layers,\n            batch_first=True,\n            dropout=dropout if num_layers > 1 else 0.0\n        )\n\n    def forward(self, input_seq):\n        embedded = self.embedding(input_seq)\n        if self.cell_type == 'LSTM':\n            outputs, (hidden, cell) = self.rnn(embedded)\n            return outputs, (hidden, cell)\n        else:\n            outputs, hidden = self.rnn(embedded)\n            return outputs, hidden","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T15:32:08.148120Z","iopub.execute_input":"2025-05-15T15:32:08.148549Z","iopub.status.idle":"2025-05-15T15:32:08.155087Z","shell.execute_reply.started":"2025-05-15T15:32:08.148530Z","shell.execute_reply":"2025-05-15T15:32:08.154474Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Decoder RNN\n\nclass DecoderRNN(nn.Module):\n    def __init__(self, output_size, embed_size, hidden_size, cell_type='RNN', num_layers=1, dropout=0.0):\n        super(DecoderRNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.embedding = nn.Embedding(output_size, embed_size)\n        self.dropout = nn.Dropout(dropout)\n        self.cell_type = cell_type.upper()\n        \n        rnn_class = {'RNN': nn.RNN, 'LSTM': nn.LSTM, 'GRU': nn.GRU}[self.cell_type]\n        self.rnn = rnn_class(\n            input_size=embed_size,\n            hidden_size=hidden_size,\n            num_layers=num_layers,\n            batch_first=True,\n            dropout=dropout if num_layers > 1 else 0.0\n        )\n        self.out = nn.Linear(hidden_size, output_size)\n\n    def forward(self, input_char, hidden):\n        embedded = self.embedding(input_char).unsqueeze(1)\n        embedded = self.dropout(embedded)\n        if self.cell_type == 'LSTM':\n            output, (hidden, cell) = self.rnn(embedded, hidden)\n            output = self.out(output.squeeze(1))\n            return output, (hidden, cell)\n        else:\n            output, hidden = self.rnn(embedded, hidden)\n            output = self.out(output.squeeze(1))\n            return output, hidden","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T15:32:08.155899Z","iopub.execute_input":"2025-05-15T15:32:08.156280Z","iopub.status.idle":"2025-05-15T15:32:08.176804Z","shell.execute_reply.started":"2025-05-15T15:32:08.156252Z","shell.execute_reply":"2025-05-15T15:32:08.176195Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Seq2Seq model\n\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder):\n        super(Seq2Seq, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, source, target, teacher_forcing_ratio=0.5):\n        batch_size = source.size(0)\n        target_len = target.size(1)\n        outputs = torch.zeros(batch_size, target_len, self.decoder.out.out_features).to(source.device)\n\n        encoder_outputs, hidden = self.encoder(source)\n        \n        decoder_input = target[:, 0]\n        \n        # Handle differing encoder/decoder layers\n        if self.encoder.cell_type == 'LSTM':\n            hidden, cell = hidden\n            if self.encoder.num_layers != self.decoder.num_layers:\n                # Compute repeat factor or select layers\n                if self.decoder.num_layers > self.encoder.num_layers:\n                    factor = self.decoder.num_layers // self.encoder.num_layers + 1\n                    hidden = hidden.repeat(factor, 1, 1)[:self.decoder.num_layers]\n                    cell = cell.repeat(factor, 1, 1)[:self.decoder.num_layers]\n\n                else:\n                    hidden = hidden[-self.decoder.num_layers:]\n                    cell = cell[-self.decoder.num_layers:]\n            hidden = (hidden, cell)\n        else:\n            # For RNN and GRU\n            if self.encoder.num_layers != self.decoder.num_layers:\n                if self.decoder.num_layers > self.encoder.num_layers:\n                    factor = self.decoder.num_layers // self.encoder.num_layers + 1\n                    hidden = hidden.repeat(factor, 1, 1)[:self.decoder.num_layers]\n                else:\n                    hidden = hidden[-self.decoder.num_layers:]\n        \n        for t in range(1, target_len):\n            output, hidden = self.decoder(decoder_input, hidden)\n            outputs[:, t, :] = output\n            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n            top1 = output.argmax(1)\n            decoder_input = target[:, t] if teacher_force else top1\n\n        return outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T15:32:08.177590Z","iopub.execute_input":"2025-05-15T15:32:08.177787Z","iopub.status.idle":"2025-05-15T15:32:08.202028Z","shell.execute_reply.started":"2025-05-15T15:32:08.177770Z","shell.execute_reply":"2025-05-15T15:32:08.201303Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Wrapper function to create model\n\ndef create_model(input_vocab_size, output_vocab_size, embed_size=256, hidden_size=512, \n                 cell_type='RNN', encoder_layers=1, decoder_layers=1, dropout=0.0):\n    encoder = EncoderRNN(input_vocab_size, embed_size, hidden_size, cell_type, encoder_layers, dropout)\n    decoder = DecoderRNN(output_vocab_size, embed_size, hidden_size, cell_type, decoder_layers, dropout)\n    model = Seq2Seq(encoder, decoder)\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T15:32:08.202943Z","iopub.execute_input":"2025-05-15T15:32:08.203338Z","iopub.status.idle":"2025-05-15T15:32:08.226171Z","shell.execute_reply.started":"2025-05-15T15:32:08.203315Z","shell.execute_reply":"2025-05-15T15:32:08.225398Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# Question 2","metadata":{}},{"cell_type":"markdown","source":"## Preparing dataset for training","metadata":{}},{"cell_type":"code","source":"# Importing Libraries\n\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch import optim\nfrom collections import Counter\nimport os\nimport math","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T15:32:08.226953Z","iopub.execute_input":"2025-05-15T15:32:08.227238Z","iopub.status.idle":"2025-05-15T15:32:08.509564Z","shell.execute_reply.started":"2025-05-15T15:32:08.227216Z","shell.execute_reply":"2025-05-15T15:32:08.508799Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Vocabulary class to handle character-to-index mapping\nclass Vocabulary:\n    def __init__(self):\n        self.char2idx = {'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3}\n        self.idx2char = {0: '<PAD>', 1: '<SOS>', 2: '<EOS>', 3: '<UNK>'}\n        self.size = 4\n\n    def add_sequence(self, sequence):\n        for char in sequence:\n            if char not in self.char2idx:\n                self.char2idx[char] = self.size\n                self.idx2char[self.size] = char\n                self.size += 1\n\n    def get_indices(self, sequence):\n        indices = [self.char2idx.get(char, self.char2idx['<UNK>']) for char in sequence]\n        return indices","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T15:32:08.511677Z","iopub.execute_input":"2025-05-15T15:32:08.511981Z","iopub.status.idle":"2025-05-15T15:32:08.517417Z","shell.execute_reply.started":"2025-05-15T15:32:08.511963Z","shell.execute_reply":"2025-05-15T15:32:08.516655Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Custom Dataset class\n\nclass DakshinaDataset(Dataset):\n    def __init__(self, data, src_vocab, tgt_vocab):\n        self.data = data\n        self.src_vocab = src_vocab\n        self.tgt_vocab = tgt_vocab\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        src = self.data.iloc[idx, 1]  # English (Latin)\n        tgt = self.data.iloc[idx, 0]  # Tamil\n        src_indices = [self.src_vocab.char2idx['<SOS>']] + self.src_vocab.get_indices(src) + [self.src_vocab.char2idx['<EOS>']]\n        tgt_indices = [self.tgt_vocab.char2idx['<SOS>']] + self.tgt_vocab.get_indices(tgt) + [self.tgt_vocab.char2idx['<EOS>']]\n        return torch.tensor(src_indices, dtype=torch.long), torch.tensor(tgt_indices, dtype=torch.long)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T15:32:08.518146Z","iopub.execute_input":"2025-05-15T15:32:08.518430Z","iopub.status.idle":"2025-05-15T15:32:08.535297Z","shell.execute_reply.started":"2025-05-15T15:32:08.518403Z","shell.execute_reply":"2025-05-15T15:32:08.534483Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Function to load and preprocess data\ndef load_dakshina_data(train_path, val_path, test_path):\n    # Read TSV files without headers\n    train_df = pd.read_csv(train_path, sep='\\t', header=None, usecols=[0, 1])\n    val_df = pd.read_csv(val_path, sep='\\t', header=None, usecols=[0, 1])\n    test_df = pd.read_csv(test_path, sep='\\t', header=None, usecols=[0, 1])\n\n    # Ensure strings\n    train_df[0] = train_df[0].astype(str)\n    train_df[1] = train_df[1].astype(str)\n    val_df[0] = val_df[0].astype(str)\n    val_df[1] = val_df[1].astype(str)\n    test_df[0] = test_df[0].astype(str)\n    test_df[1] = test_df[1].astype(str)\n\n    # Build vocabularies\n    src_vocab = Vocabulary()  # English (Latin)\n    tgt_vocab = Vocabulary()  # Tamil\n\n    # Add characters to vocab from training data\n    for _, row in train_df.iterrows():\n        src_vocab.add_sequence(row[1])\n        tgt_vocab.add_sequence(row[0])\n\n    # Create datasets\n    train_dataset = DakshinaDataset(train_df, src_vocab, tgt_vocab)\n    val_dataset = DakshinaDataset(val_df, src_vocab, tgt_vocab)\n    test_dataset = DakshinaDataset(test_df, src_vocab, tgt_vocab)\n\n    return train_dataset, val_dataset, test_dataset, src_vocab, tgt_vocab\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T15:32:08.536095Z","iopub.execute_input":"2025-05-15T15:32:08.536416Z","iopub.status.idle":"2025-05-15T15:32:08.553861Z","shell.execute_reply.started":"2025-05-15T15:32:08.536390Z","shell.execute_reply":"2025-05-15T15:32:08.553089Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Collate function for DataLoader\ndef collate_fn(batch):\n    src_batch, tgt_batch = zip(*batch)\n    # Pad sequences\n    src_padded = torch.nn.utils.rnn.pad_sequence(src_batch, batch_first=True, padding_value=0)\n    tgt_padded = torch.nn.utils.rnn.pad_sequence(tgt_batch, batch_first=True, padding_value=0)\n    return src_padded, tgt_padded","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T15:32:08.554569Z","iopub.execute_input":"2025-05-15T15:32:08.554807Z","iopub.status.idle":"2025-05-15T15:32:08.572850Z","shell.execute_reply.started":"2025-05-15T15:32:08.554791Z","shell.execute_reply":"2025-05-15T15:32:08.572086Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Wrapper function for easier access\n\ndef prepare_data_loaders(train_path, val_path, test_path, batch_size=32):\n    train_dataset, val_dataset, test_dataset, src_vocab, tgt_vocab = load_dakshina_data(train_path, val_path, test_path)\n    \n    train_loader = DataLoader(\n        train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, pin_memory=True\n    )\n    val_loader = DataLoader(\n        val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, pin_memory=True\n    )\n    test_loader = DataLoader(\n        test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, pin_memory=True\n    )\n    \n    return train_loader, val_loader, test_loader, src_vocab, tgt_vocab","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T15:32:08.573601Z","iopub.execute_input":"2025-05-15T15:32:08.573857Z","iopub.status.idle":"2025-05-15T15:32:08.589398Z","shell.execute_reply.started":"2025-05-15T15:32:08.573837Z","shell.execute_reply":"2025-05-15T15:32:08.588673Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Paths to your local TSV files (update as needed)\ntrain_path = '/kaggle/input/dakshina-tamil/ta.translit.sampled.train.tsv'\nval_path = '/kaggle/input/dakshina-tamil/ta.translit.sampled.dev.tsv'\ntest_path = '/kaggle/input/dakshina-tamil/ta.translit.sampled.test.tsv'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T15:32:08.590073Z","iopub.execute_input":"2025-05-15T15:32:08.590480Z","iopub.status.idle":"2025-05-15T15:32:08.609408Z","shell.execute_reply.started":"2025-05-15T15:32:08.590431Z","shell.execute_reply":"2025-05-15T15:32:08.608599Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Create data loaders\ntrain_loader, val_loader, test_loader, src_vocab, tgt_vocab = prepare_data_loaders(train_path, val_path, test_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T15:32:08.610197Z","iopub.execute_input":"2025-05-15T15:32:08.610429Z","iopub.status.idle":"2025-05-15T15:32:11.501163Z","shell.execute_reply.started":"2025-05-15T15:32:08.610413Z","shell.execute_reply":"2025-05-15T15:32:11.500343Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Print vocabulary sizes\nprint(f\"Source (English) vocabulary size: {src_vocab.size}\")\nprint(f\"Target (Tamil) vocabulary size: {tgt_vocab.size}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T15:32:11.501846Z","iopub.execute_input":"2025-05-15T15:32:11.502044Z","iopub.status.idle":"2025-05-15T15:32:11.506691Z","shell.execute_reply.started":"2025-05-15T15:32:11.502028Z","shell.execute_reply":"2025-05-15T15:32:11.505853Z"}},"outputs":[{"name":"stdout","text":"Source (English) vocabulary size: 30\nTarget (Tamil) vocabulary size: 50\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"## Setting up wandb","metadata":{}},{"cell_type":"code","source":"!pip install wandb -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T15:32:11.507351Z","iopub.execute_input":"2025-05-15T15:32:11.507591Z","iopub.status.idle":"2025-05-15T15:32:15.356098Z","shell.execute_reply.started":"2025-05-15T15:32:11.507575Z","shell.execute_reply":"2025-05-15T15:32:15.355314Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"import wandb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T15:32:15.357229Z","iopub.execute_input":"2025-05-15T15:32:15.357500Z","iopub.status.idle":"2025-05-15T15:32:18.044887Z","shell.execute_reply.started":"2025-05-15T15:32:15.357475Z","shell.execute_reply":"2025-05-15T15:32:18.044344Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"wandb\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T15:32:18.045630Z","iopub.execute_input":"2025-05-15T15:32:18.046046Z","iopub.status.idle":"2025-05-15T15:32:18.110898Z","shell.execute_reply.started":"2025-05-15T15:32:18.046027Z","shell.execute_reply":"2025-05-15T15:32:18.110171Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"wandb.login(key=secret_value_0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T15:32:18.111697Z","iopub.execute_input":"2025-05-15T15:32:18.111939Z","iopub.status.idle":"2025-05-15T15:32:24.188823Z","shell.execute_reply.started":"2025-05-15T15:32:18.111916Z","shell.execute_reply":"2025-05-15T15:32:24.188275Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mda24m007\u001b[0m (\u001b[33mda24m007-iit-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":18},{"cell_type":"markdown","source":"## Running wandb sweep","metadata":{}},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T15:32:24.189516Z","iopub.execute_input":"2025-05-15T15:32:24.189991Z","iopub.status.idle":"2025-05-15T15:32:24.258368Z","shell.execute_reply.started":"2025-05-15T15:32:24.189964Z","shell.execute_reply":"2025-05-15T15:32:24.257636Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# Beam search decoding\ndef beam_search_decode(model, src, max_len, beam_width, sos_idx, eos_idx):\n    model.eval()\n    src = src.to(device)\n    batch_size = src.size(0)\n    encoder_outputs, hidden = model.encoder(src)\n    \n    # Adjust hidden state to match decoder layers\n    if model.encoder.cell_type == 'LSTM':\n        hidden, cell = hidden\n        if model.encoder.num_layers != model.decoder.num_layers:\n            factor = model.decoder.num_layers // model.encoder.num_layers\n            if factor > 1:\n                hidden = hidden.repeat(factor, 1, 1)\n                cell = cell.repeat(factor, 1, 1)\n            else:\n                hidden = hidden[-model.decoder.num_layers:]\n                cell = cell[-model.decoder.num_layers:]\n        hidden = (hidden, cell)\n    else:\n        if model.encoder.num_layers != model.decoder.num_layers:\n            factor = model.decoder.num_layers // model.encoder.num_layers\n            if factor > 1:\n                hidden = hidden.repeat(factor, 1, 1)\n            else:\n                hidden = hidden[-model.decoder.num_layers:]\n    \n    # Initialize beam\n    beams = [(torch.tensor([sos_idx], device=device), hidden, 0.0)]  # (sequence, hidden, score)\n    completed = []\n    \n    for _ in range(max_len):\n        new_beams = []\n        for seq, hid, score in beams:\n            if seq[-1].item() == eos_idx:\n                completed.append((seq, score))\n                continue\n            output, new_hidden = model.decoder(seq[-1].unsqueeze(0), hid)\n            probs = torch.softmax(output, dim=-1)\n            top_probs, top_idx = probs.topk(beam_width)\n            \n            for i in range(beam_width):\n                new_seq = torch.cat([seq, top_idx[:, i]])\n                new_score = score - math.log(top_probs[:, i].item())\n                new_beams.append((new_seq, new_hidden, new_score))\n        \n        # Keep top beam_width beams\n        new_beams = sorted(new_beams, key=lambda x: x[2])[:beam_width]\n        beams = new_beams\n        \n        if len(completed) >= beam_width:\n            break\n    \n    # Return best sequence\n    completed = sorted(completed, key=lambda x: x[1])\n    if completed:\n        return completed[0][0]\n    return beams[0][0]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T15:32:24.259185Z","iopub.execute_input":"2025-05-15T15:32:24.259441Z","iopub.status.idle":"2025-05-15T15:32:24.276840Z","shell.execute_reply.started":"2025-05-15T15:32:24.259416Z","shell.execute_reply":"2025-05-15T15:32:24.276169Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# Training and evaluation function\ndef train_and_evaluate():\n    wandb.init()\n    config = wandb.config\n    \n    # Create model with sweep parameters\n    model = create_model(\n        input_vocab_size=src_vocab.size,\n        output_vocab_size=tgt_vocab.size,\n        embed_size=config.embed_size,\n        hidden_size=config.hidden_size,\n        cell_type=config.cell_type,\n        encoder_layers=config.encoder_layers,\n        decoder_layers=config.decoder_layers,\n        dropout=config.dropout\n    ).to(device)\n    \n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    criterion = nn.CrossEntropyLoss(ignore_index=0)\n    \n    for epoch in range(10):\n        model.train()\n        train_loss = 0\n        for src, tgt in train_loader:\n            src, tgt = src.to(device), tgt.to(device)\n            optimizer.zero_grad()\n            output = model(src, tgt, teacher_forcing_ratio=0.5)\n            output = output[:, 1:].reshape(-1, output.size(-1))\n            tgt = tgt[:, 1:].reshape(-1)\n            loss = criterion(output, tgt)\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n        \n        # Compute training accuracy on one batch\n        model.eval()\n        train_correct = 0\n        train_total = 0\n        with torch.no_grad():\n            for src, tgt in train_loader:\n                src, tgt = src.to(device), tgt.to(device)\n                for i in range(src.size(0)):\n                    pred = beam_search_decode(\n                        model, src[i:i+1], max_len=50,\n                        beam_width=config.beam_width,\n                        sos_idx=tgt_vocab.char2idx['<SOS>'],\n                        eos_idx=tgt_vocab.char2idx['<EOS>']\n                    )\n                    pred_str = ''.join([tgt_vocab.idx2char[idx.item()] for idx in pred if idx.item() not in [0, 1, 2]])\n                    tgt_str = ''.join([tgt_vocab.idx2char[idx.item()] for idx in tgt[i, 1:] if idx.item() not in [0, 1, 2]])\n                    if pred_str == tgt_str:\n                        train_correct += 1\n                    train_total += 1\n                break\n        \n        # Compute validation loss and accuracy\n        val_loss = 0\n        val_correct = 0\n        val_total = 0\n        with torch.no_grad():\n            for src, tgt in val_loader:\n                src, tgt = src.to(device), tgt.to(device)\n                output = model(src, tgt, teacher_forcing_ratio=0.0)\n                output = output[:, 1:].reshape(-1, output.size(-1))\n                tgt = tgt[:, 1:].reshape(-1)\n                loss = criterion(output, tgt)\n                val_loss += loss.item()\n            \n            # Validation accuracy on one batch\n            for src, tgt in val_loader:\n                src, tgt = src.to(device), tgt.to(device)\n                for i in range(src.size(0)):\n                    pred = beam_search_decode(\n                        model, src[i:i+1], max_len=50,\n                        beam_width=config.beam_width,\n                        sos_idx=tgt_vocab.char2idx['<SOS>'],\n                        eos_idx=tgt_vocab.char2idx['<EOS>']\n                    )\n                    pred_str = ''.join([tgt_vocab.idx2char[idx.item()] for idx in pred if idx.item() not in [0, 1, 2]])\n                    tgt_str = ''.join([tgt_vocab.idx2char[idx.item()] for idx in tgt[i, 1:] if idx.item() not in [0, 1, 2]])\n                    if pred_str == tgt_str:\n                        val_correct += 1\n                    val_total += 1\n                break\n        \n        # Log metrics to WandB\n        wandb.log({\n            \"epoch\": epoch,\n            \"train_loss\": train_loss / len(train_loader),\n            \"val_loss\": val_loss / len(val_loader),\n            \"train_accuracy\": train_correct / train_total,\n            \"val_accuracy\": val_correct / val_total\n        })","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T15:32:24.277530Z","iopub.execute_input":"2025-05-15T15:32:24.277784Z","iopub.status.idle":"2025-05-15T15:32:24.297987Z","shell.execute_reply.started":"2025-05-15T15:32:24.277768Z","shell.execute_reply":"2025-05-15T15:32:24.297357Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# WandB sweep configuration\nsweep_config = {\n    'method': 'bayes',\n    'metric': {\n        'name': 'val_accuracy',\n        'goal': 'maximize'\n    },\n    'parameters': {\n        'embed_size': {\n            'values': [16,32,64, 128, 256]\n        },\n        'hidden_size': {\n            'values': [16,32,128, 256, 512]\n        },\n        'encoder_layers': {\n            'values': [1, 2, 3]\n        },\n        'decoder_layers': {\n            'values': [1, 2, 3]\n        },\n        'cell_type': {\n            'values': ['RNN', 'GRU', 'LSTM']\n        },\n        'dropout': {\n            'values': [0.2, 0.3]\n        },\n        'beam_width': {\n            'values': [1, 3, 5]\n        }\n    }\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T15:32:24.300647Z","iopub.execute_input":"2025-05-15T15:32:24.300845Z","iopub.status.idle":"2025-05-15T15:32:24.320071Z","shell.execute_reply.started":"2025-05-15T15:32:24.300830Z","shell.execute_reply":"2025-05-15T15:32:24.319459Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# Initialize and run sweep\nsweep_id = wandb.sweep(sweep_config, project=\"DL-A3\")\nwandb.agent(sweep_id, function=train_and_evaluate, count=20)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T06:12:39.951986Z","iopub.execute_input":"2025-05-15T06:12:39.952216Z","execution_failed":"2025-05-15T06:16:19.342Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Question 4","metadata":{}},{"cell_type":"code","source":"wandb.init(project=\"DL-A3\", name=\"final_model_training\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T15:46:41.394639Z","iopub.execute_input":"2025-05-15T15:46:41.394915Z","iopub.status.idle":"2025-05-15T15:46:47.787263Z","shell.execute_reply.started":"2025-05-15T15:46:41.394896Z","shell.execute_reply":"2025-05-15T15:46:47.786628Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250515_154641-lpuvod0h</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/da24m007-iit-madras/DL-A3/runs/lpuvod0h' target=\"_blank\">final_model_training</a></strong> to <a href='https://wandb.ai/da24m007-iit-madras/DL-A3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/da24m007-iit-madras/DL-A3' target=\"_blank\">https://wandb.ai/da24m007-iit-madras/DL-A3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/da24m007-iit-madras/DL-A3/runs/lpuvod0h' target=\"_blank\">https://wandb.ai/da24m007-iit-madras/DL-A3/runs/lpuvod0h</a>"},"metadata":{}},{"execution_count":34,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/da24m007-iit-madras/DL-A3/runs/lpuvod0h?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x79157f0d4390>"},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"# Function to retrieve best hyperparameters from WandB sweep\ndef get_best_hyperparameters(project_name=\"DL-A3\"):\n    api = wandb.Api()\n    runs = api.runs(project_name)\n    best_run = None\n    best_val_accuracy = -float('inf')\n    \n    for run in runs:\n        if 'val_accuracy' in run.summary and run.summary['val_accuracy'] > best_val_accuracy:\n            best_val_accuracy = run.summary['val_accuracy']\n            best_run = run\n    \n    if best_run is None:\n        raise ValueError(\"No runs found with val_accuracy in WandB project\")\n    \n    # Extract hyperparameters\n    config = best_run.config\n    return {\n        'embed_size': config['embed_size'],\n        'hidden_size': config['hidden_size'],\n        'cell_type': config['cell_type'],\n        'encoder_layers': config['encoder_layers'],\n        'decoder_layers': config['decoder_layers'],\n        'dropout': config['dropout'],\n        'beam_width': config['beam_width']\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T15:32:53.178935Z","iopub.execute_input":"2025-05-15T15:32:53.179254Z","iopub.status.idle":"2025-05-15T15:32:53.184546Z","shell.execute_reply.started":"2025-05-15T15:32:53.179233Z","shell.execute_reply":"2025-05-15T15:32:53.183969Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# Beam search decoding\ndef beam_search_decode(model, src, max_len, beam_width, sos_idx, eos_idx):\n    model.eval()\n    src = src.to(device)\n    batch_size = src.size(0)\n    encoder_outputs, hidden = model.encoder(src)\n    \n    if model.encoder.cell_type == 'LSTM':\n        hidden, cell = hidden\n        if model.encoder.num_layers != model.decoder.num_layers:\n            factor = model.decoder.num_layers // model.encoder.num_layers\n            if factor > 1:\n                hidden = hidden.repeat(factor, 1, 1)\n                cell = cell.repeat(factor, 1, 1)\n            else:\n                hidden = hidden[-model.decoder.num_layers:]\n                cell = cell[-model.decoder.num_layers:]\n        hidden = (hidden, cell)\n    else:\n        if model.encoder.num_layers != model.decoder.num_layers:\n            factor = model.decoder.num_layers // model.encoder.num_layers\n            if factor > 1:\n                hidden = hidden.repeat(factor, 1, 1)\n            else:\n                hidden = hidden[-model.decoder.num_layers:]\n    \n    beams = [(torch.tensor([sos_idx], device=device), hidden, 0.0)]\n    completed = []\n    \n    for _ in range(max_len):\n        new_beams = []\n        for seq, hid, score in beams:\n            if seq[-1].item() == eos_idx:\n                completed.append((seq, score))\n                continue\n            output, new_hidden = model.decoder(seq[-1].unsqueeze(0), hid)\n            probs = torch.softmax(output, dim=-1)\n            top_probs, top_idx = probs.topk(beam_width)\n            \n            for i in range(beam_width):\n                new_seq = torch.cat([seq, top_idx[:, i]])\n                new_score = score - math.log(top_probs[:, i].item())\n                new_beams.append((new_seq, new_hidden, new_score))\n        \n        new_beams = sorted(new_beams, key=lambda x: x[2])[:beam_width]\n        beams = new_beams\n        \n        if len(completed) >= beam_width:\n            break\n    \n    completed = sorted(completed, key=lambda x: x[1])\n    if completed:\n        return completed[0][0]\n    return beams[0][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T15:46:20.082567Z","iopub.execute_input":"2025-05-15T15:46:20.083155Z","iopub.status.idle":"2025-05-15T15:46:20.091764Z","shell.execute_reply.started":"2025-05-15T15:46:20.083116Z","shell.execute_reply":"2025-05-15T15:46:20.090996Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"# Training and evaluation function\ndef train_model(model, train_loader, val_loader, test_loader, num_epochs=30, beam_width=3):\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    criterion = nn.CrossEntropyLoss(ignore_index=0)\n    \n    for epoch in range(num_epochs):\n        model.train()\n        train_loss = 0\n        for src, tgt in train_loader:\n            src, tgt = src.to(device), tgt.to(device)\n            optimizer.zero_grad()\n            output = model(src, tgt, teacher_forcing_ratio=0.5)\n            output = output[:, 1:].reshape(-1, output.size(-1))\n            tgt_flat = tgt[:, 1:].reshape(-1)\n            loss = criterion(output, tgt_flat)\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n        \n        # Compute validation loss and accuracy (one batch for speed)\n        model.eval()\n        val_loss = 0\n        val_correct = 0\n        val_total = 0\n        with torch.no_grad():\n            for src, tgt in val_loader:\n                src, tgt = src.to(device), tgt.to(device)\n                output = model(src, tgt, teacher_forcing_ratio=0.0)\n                output = output[:, 1:].reshape(-1, output.size(-1))\n                tgt_flat = tgt[:, 1:].reshape(-1)\n                loss = criterion(output, tgt_flat)\n                val_loss += loss.item()\n                \n                # Compute accuracy on one batch\n                print(f\"Epoch {epoch+1}, Validation batch - src shape: {src.shape}, tgt shape: {tgt.shape}\")\n                for i in range(src.size(0)):\n                    pred = beam_search_decode(\n                        model, src[i:i+1], max_len=50,\n                        beam_width=beam_width,\n                        sos_idx=tgt_vocab.char2idx['<SOS>'],\n                        eos_idx=tgt_vocab.char2idx['<EOS>']\n                    )\n                    pred_str = ''.join([tgt_vocab.idx2char[idx.item()] for idx in pred if idx.item() not in [0, 1, 2]])\n                    tgt_seq = tgt[i]\n                    tgt_str = ''.join([tgt_vocab.idx2char[idx.item()] for idx in tgt_seq[1:] if idx.item() not in [0, 1, 2]])\n                    if pred_str == tgt_str:\n                        val_correct += 1\n                    val_total += 1\n                break  # Process only one batch for validation accuracy\n        \n        # Log metrics to WandB\n        wandb.log({\n            \"epoch\": epoch,\n            \"train_loss\": train_loss / len(train_loader),\n            \"val_loss\": val_loss / len(val_loader),\n            \"val_accuracy\": val_correct / val_total\n        })\n        print(f\"Epoch {epoch+1}, Train Loss: {train_loss / len(train_loader):.4f}, Val Loss: {val_loss / len(val_loader):.4f}, Val Accuracy: {val_correct / val_total:.4f}\")\n    \n    # Compute test accuracy and save predictions\n    test_correct = 0\n    test_total = 0\n    predictions = []\n    with torch.no_grad():\n        for src, tgt in test_loader:\n            src, tgt = src.to(device), tgt.to(device)\n            print(f\"Test batch - src shape: {src.shape}, tgt shape: {tgt.shape}\")\n            for i in range(src.size(0)):\n                pred = beam_search_decode(\n                    model, src[i:i+1], max_len=50,\n                    beam_width=beam_width,\n                    sos_idx=tgt_vocab.char2idx['<SOS>'],\n                    eos_idx=tgt_vocab.char2idx['<EOS>']\n                )\n                pred_str = ''.join([tgt_vocab.idx2char[idx.item()] for idx in pred if idx.item() not in [0, 1, 2]])\n                tgt_seq = tgt[i]\n                tgt_str = ''.join([tgt_vocab.idx2char[idx.item()] for idx in tgt_seq[1:] if idx.item() not in [0, 1, 2]])\n                src_str = ''.join([src_vocab.idx2char[idx.item()] for idx in src[i, 1:] if idx.item() not in [0, 1, 2]])\n                if pred_str == tgt_str:\n                    test_correct += 1\n                test_total += 1\n                predictions.append({\n                    'input_english': src_str,\n                    'actual_tamil': tgt_str,\n                    'predicted_tamil': pred_str\n                })\n    \n    test_accuracy = test_correct / test_total\n    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n    \n    # Save predictions to CSV\n    predictions_df = pd.DataFrame(predictions)\n    predictions_df.to_csv('test_predictions_vanilla.csv', index=False)\n    print(\"Test predictions saved to 'test_predictions.csv'\")\n    \n    return test_accuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T15:47:33.810300Z","iopub.execute_input":"2025-05-15T15:47:33.810589Z","iopub.status.idle":"2025-05-15T15:47:33.824737Z","shell.execute_reply.started":"2025-05-15T15:47:33.810570Z","shell.execute_reply":"2025-05-15T15:47:33.824066Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"# Get best hyperparameters\ntry:\n    best_params = get_best_hyperparameters()\n    print(\"Best hyperparameters from WandB sweep:\", best_params)\nexcept Exception as e:\n    print(f\"Error retrieving hyperparameters: {e}\")\n    print(\"Using default hyperparameters as fallback\")\n    best_params = {\n        'embed_size': 256,\n        'hidden_size': 512,\n        'cell_type': 'LSTM',\n        'encoder_layers': 2,\n        'decoder_layers': 1,\n        'dropout': 0.2,\n        'beam_width': 5\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T15:41:47.710398Z","iopub.execute_input":"2025-05-15T15:41:47.711015Z","iopub.status.idle":"2025-05-15T15:41:48.684810Z","shell.execute_reply.started":"2025-05-15T15:41:47.710992Z","shell.execute_reply":"2025-05-15T15:41:48.684023Z"}},"outputs":[{"name":"stdout","text":"Best hyperparameters from WandB sweep: {'embed_size': 32, 'hidden_size': 256, 'cell_type': 'LSTM', 'encoder_layers': 1, 'decoder_layers': 3, 'dropout': 0.3, 'beam_width': 3}\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"# Create model with best parameters\nmodel = create_model(\n    input_vocab_size=src_vocab.size,\n    output_vocab_size=tgt_vocab.size,\n    embed_size=best_params['embed_size'],\n    hidden_size=best_params['hidden_size'],\n    cell_type=best_params['cell_type'],\n    encoder_layers=best_params['encoder_layers'],\n    decoder_layers=best_params['decoder_layers'],\n    dropout=best_params['dropout']\n).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T15:47:38.314741Z","iopub.execute_input":"2025-05-15T15:47:38.315372Z","iopub.status.idle":"2025-05-15T15:47:38.341955Z","shell.execute_reply.started":"2025-05-15T15:47:38.315351Z","shell.execute_reply":"2025-05-15T15:47:38.341401Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"# Train and evaluate\ntest_accuracy = train_model(\n    model, train_loader, val_loader, test_loader,\n    num_epochs=30, beam_width=best_params['beam_width']\n)\n\n# Log test accuracy to WandB\nwandb.log({\"test_accuracy\": test_accuracy})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T15:47:41.241736Z","iopub.execute_input":"2025-05-15T15:47:41.242319Z","iopub.status.idle":"2025-05-15T16:34:17.646668Z","shell.execute_reply.started":"2025-05-15T15:47:41.242300Z","shell.execute_reply":"2025-05-15T16:34:17.646076Z"}},"outputs":[{"name":"stdout","text":"Epoch 1, Validation batch - src shape: torch.Size([32, 20]), tgt shape: torch.Size([32, 15])\nEpoch 1, Train Loss: 1.7106, Val Loss: 0.0085, Val Accuracy: 0.0625\nEpoch 2, Validation batch - src shape: torch.Size([32, 20]), tgt shape: torch.Size([32, 15])\nEpoch 2, Train Loss: 0.5837, Val Loss: 0.0074, Val Accuracy: 0.3438\nEpoch 3, Validation batch - src shape: torch.Size([32, 20]), tgt shape: torch.Size([32, 15])\nEpoch 3, Train Loss: 0.3993, Val Loss: 0.0066, Val Accuracy: 0.4688\nEpoch 4, Validation batch - src shape: torch.Size([32, 20]), tgt shape: torch.Size([32, 15])\nEpoch 4, Train Loss: 0.3166, Val Loss: 0.0075, Val Accuracy: 0.3750\nEpoch 5, Validation batch - src shape: torch.Size([32, 20]), tgt shape: torch.Size([32, 15])\nEpoch 5, Train Loss: 0.2653, Val Loss: 0.0075, Val Accuracy: 0.3438\nEpoch 6, Validation batch - src shape: torch.Size([32, 20]), tgt shape: torch.Size([32, 15])\nEpoch 6, Train Loss: 0.2254, Val Loss: 0.0077, Val Accuracy: 0.4062\nEpoch 7, Validation batch - src shape: torch.Size([32, 20]), tgt shape: torch.Size([32, 15])\nEpoch 7, Train Loss: 0.1985, Val Loss: 0.0084, Val Accuracy: 0.4062\nEpoch 8, Validation batch - src shape: torch.Size([32, 20]), tgt shape: torch.Size([32, 15])\nEpoch 8, Train Loss: 0.1770, Val Loss: 0.0074, Val Accuracy: 0.4062\nEpoch 9, Validation batch - src shape: torch.Size([32, 20]), tgt shape: torch.Size([32, 15])\nEpoch 9, Train Loss: 0.1588, Val Loss: 0.0080, Val Accuracy: 0.4062\nEpoch 10, Validation batch - src shape: torch.Size([32, 20]), tgt shape: torch.Size([32, 15])\nEpoch 10, Train Loss: 0.1460, Val Loss: 0.0087, Val Accuracy: 0.3438\nEpoch 11, Validation batch - src shape: torch.Size([32, 20]), tgt shape: torch.Size([32, 15])\nEpoch 11, Train Loss: 0.1346, Val Loss: 0.0078, Val Accuracy: 0.5312\nEpoch 12, Validation batch - src shape: torch.Size([32, 20]), tgt shape: torch.Size([32, 15])\nEpoch 12, Train Loss: 0.1214, Val Loss: 0.0084, Val Accuracy: 0.4375\nEpoch 13, Validation batch - src shape: torch.Size([32, 20]), tgt shape: torch.Size([32, 15])\nEpoch 13, Train Loss: 0.1156, Val Loss: 0.0103, Val Accuracy: 0.3438\nEpoch 14, Validation batch - src shape: torch.Size([32, 20]), tgt shape: torch.Size([32, 15])\nEpoch 14, Train Loss: 0.1069, Val Loss: 0.0090, Val Accuracy: 0.4375\nEpoch 15, Validation batch - src shape: torch.Size([32, 20]), tgt shape: torch.Size([32, 15])\nEpoch 15, Train Loss: 0.1035, Val Loss: 0.0097, Val Accuracy: 0.3750\nEpoch 16, Validation batch - src shape: torch.Size([32, 20]), tgt shape: torch.Size([32, 15])\nEpoch 16, Train Loss: 0.0985, Val Loss: 0.0097, Val Accuracy: 0.4062\nEpoch 17, Validation batch - src shape: torch.Size([32, 20]), tgt shape: torch.Size([32, 15])\nEpoch 17, Train Loss: 0.0922, Val Loss: 0.0098, Val Accuracy: 0.3438\nEpoch 18, Validation batch - src shape: torch.Size([32, 20]), tgt shape: torch.Size([32, 15])\nEpoch 18, Train Loss: 0.0893, Val Loss: 0.0097, Val Accuracy: 0.3750\nEpoch 19, Validation batch - src shape: torch.Size([32, 20]), tgt shape: torch.Size([32, 15])\nEpoch 19, Train Loss: 0.0854, Val Loss: 0.0092, Val Accuracy: 0.4062\nEpoch 20, Validation batch - src shape: torch.Size([32, 20]), tgt shape: torch.Size([32, 15])\nEpoch 20, Train Loss: 0.0837, Val Loss: 0.0097, Val Accuracy: 0.4688\nEpoch 21, Validation batch - src shape: torch.Size([32, 20]), tgt shape: torch.Size([32, 15])\nEpoch 21, Train Loss: 0.0797, Val Loss: 0.0110, Val Accuracy: 0.3750\nEpoch 22, Validation batch - src shape: torch.Size([32, 20]), tgt shape: torch.Size([32, 15])\nEpoch 22, Train Loss: 0.0794, Val Loss: 0.0100, Val Accuracy: 0.3750\nEpoch 23, Validation batch - src shape: torch.Size([32, 20]), tgt shape: torch.Size([32, 15])\nEpoch 23, Train Loss: 0.0757, Val Loss: 0.0095, Val Accuracy: 0.5312\nEpoch 24, Validation batch - src shape: torch.Size([32, 20]), tgt shape: torch.Size([32, 15])\nEpoch 24, Train Loss: 0.0733, Val Loss: 0.0106, Val Accuracy: 0.3125\nEpoch 25, Validation batch - src shape: torch.Size([32, 20]), tgt shape: torch.Size([32, 15])\nEpoch 25, Train Loss: 0.0721, Val Loss: 0.0107, Val Accuracy: 0.3750\nEpoch 26, Validation batch - src shape: torch.Size([32, 20]), tgt shape: torch.Size([32, 15])\nEpoch 26, Train Loss: 0.0695, Val Loss: 0.0110, Val Accuracy: 0.3438\nEpoch 27, Validation batch - src shape: torch.Size([32, 20]), tgt shape: torch.Size([32, 15])\nEpoch 27, Train Loss: 0.0689, Val Loss: 0.0114, Val Accuracy: 0.3438\nEpoch 28, Validation batch - src shape: torch.Size([32, 20]), tgt shape: torch.Size([32, 15])\nEpoch 28, Train Loss: 0.0673, Val Loss: 0.0112, Val Accuracy: 0.3125\nEpoch 29, Validation batch - src shape: torch.Size([32, 20]), tgt shape: torch.Size([32, 15])\nEpoch 29, Train Loss: 0.0660, Val Loss: 0.0114, Val Accuracy: 0.3750\nEpoch 30, Validation batch - src shape: torch.Size([32, 20]), tgt shape: torch.Size([32, 15])\nEpoch 30, Train Loss: 0.0651, Val Loss: 0.0110, Val Accuracy: 0.3438\nTest batch - src shape: torch.Size([32, 20]), tgt shape: torch.Size([32, 15])\nTest batch - src shape: torch.Size([32, 23]), tgt shape: torch.Size([32, 19])\nTest batch - src shape: torch.Size([32, 15]), tgt shape: torch.Size([32, 15])\nTest batch - src shape: torch.Size([32, 19]), tgt shape: torch.Size([32, 17])\nTest batch - src shape: torch.Size([32, 19]), tgt shape: torch.Size([32, 17])\nTest batch - src shape: torch.Size([32, 20]), tgt shape: torch.Size([32, 17])\nTest batch - src shape: torch.Size([32, 21]), tgt shape: torch.Size([32, 17])\nTest batch - src shape: torch.Size([32, 18]), tgt shape: torch.Size([32, 19])\nTest batch - src shape: torch.Size([32, 16]), tgt shape: torch.Size([32, 17])\nTest batch - src shape: torch.Size([32, 16]), tgt shape: torch.Size([32, 17])\nTest batch - src shape: torch.Size([32, 17]), tgt shape: torch.Size([32, 14])\nTest batch - src shape: torch.Size([32, 17]), tgt shape: torch.Size([32, 12])\nTest batch - src shape: torch.Size([32, 18]), tgt shape: torch.Size([32, 14])\nTest batch - src shape: torch.Size([32, 19]), tgt shape: torch.Size([32, 15])\nTest batch - src shape: torch.Size([32, 18]), tgt shape: torch.Size([32, 15])\nTest batch - src shape: torch.Size([32, 18]), tgt shape: torch.Size([32, 15])\nTest batch - src shape: torch.Size([32, 18]), tgt shape: torch.Size([32, 19])\nTest batch - src shape: torch.Size([32, 17]), tgt shape: torch.Size([32, 16])\nTest batch - src shape: torch.Size([32, 19]), tgt shape: torch.Size([32, 19])\nTest batch - src shape: torch.Size([32, 16]), tgt shape: torch.Size([32, 13])\nTest batch - src shape: torch.Size([32, 21]), tgt shape: torch.Size([32, 16])\nTest batch - src shape: torch.Size([32, 22]), tgt shape: torch.Size([32, 19])\nTest batch - src shape: torch.Size([32, 19]), tgt shape: torch.Size([32, 17])\nTest batch - src shape: torch.Size([32, 14]), tgt shape: torch.Size([32, 13])\nTest batch - src shape: torch.Size([32, 16]), tgt shape: torch.Size([32, 14])\nTest batch - src shape: torch.Size([32, 20]), tgt shape: torch.Size([32, 17])\nTest batch - src shape: torch.Size([32, 19]), tgt shape: torch.Size([32, 14])\nTest batch - src shape: torch.Size([32, 14]), tgt shape: torch.Size([32, 13])\nTest batch - src shape: torch.Size([32, 15]), tgt shape: torch.Size([32, 14])\nTest batch - src shape: torch.Size([32, 14]), tgt shape: torch.Size([32, 13])\nTest batch - src shape: torch.Size([32, 19]), tgt shape: torch.Size([32, 18])\nTest batch - src shape: torch.Size([32, 17]), tgt shape: torch.Size([32, 18])\nTest batch - src shape: torch.Size([32, 14]), tgt shape: torch.Size([32, 14])\nTest batch - src shape: torch.Size([32, 18]), tgt shape: torch.Size([32, 17])\nTest batch - src shape: torch.Size([32, 17]), tgt shape: torch.Size([32, 18])\nTest batch - src shape: torch.Size([32, 14]), tgt shape: torch.Size([32, 15])\nTest batch - src shape: torch.Size([32, 17]), tgt shape: torch.Size([32, 16])\nTest batch - src shape: torch.Size([32, 18]), tgt shape: torch.Size([32, 16])\nTest batch - src shape: torch.Size([32, 19]), tgt shape: torch.Size([32, 14])\nTest batch - src shape: torch.Size([32, 18]), tgt shape: torch.Size([32, 17])\nTest batch - src shape: torch.Size([32, 18]), tgt shape: torch.Size([32, 17])\nTest batch - src shape: torch.Size([32, 16]), tgt shape: torch.Size([32, 15])\nTest batch - src shape: torch.Size([32, 22]), tgt shape: torch.Size([32, 19])\nTest batch - src shape: torch.Size([32, 19]), tgt shape: torch.Size([32, 18])\nTest batch - src shape: torch.Size([32, 21]), tgt shape: torch.Size([32, 20])\nTest batch - src shape: torch.Size([32, 21]), tgt shape: torch.Size([32, 18])\nTest batch - src shape: torch.Size([32, 15]), tgt shape: torch.Size([32, 13])\nTest batch - src shape: torch.Size([32, 17]), tgt shape: torch.Size([32, 17])\nTest batch - src shape: torch.Size([32, 22]), tgt shape: torch.Size([32, 21])\nTest batch - src shape: torch.Size([32, 15]), tgt shape: torch.Size([32, 15])\nTest batch - src shape: torch.Size([32, 17]), tgt shape: torch.Size([32, 14])\nTest batch - src shape: torch.Size([32, 21]), tgt shape: torch.Size([32, 16])\nTest batch - src shape: torch.Size([32, 15]), tgt shape: torch.Size([32, 15])\nTest batch - src shape: torch.Size([32, 20]), tgt shape: torch.Size([32, 17])\nTest batch - src shape: torch.Size([32, 18]), tgt shape: torch.Size([32, 15])\nTest batch - src shape: torch.Size([32, 19]), tgt shape: torch.Size([32, 16])\nTest batch - src shape: torch.Size([32, 13]), tgt shape: torch.Size([32, 12])\nTest batch - src shape: torch.Size([32, 19]), tgt shape: torch.Size([32, 17])\nTest batch - src shape: torch.Size([32, 16]), tgt shape: torch.Size([32, 15])\nTest batch - src shape: torch.Size([32, 13]), tgt shape: torch.Size([32, 12])\nTest batch - src shape: torch.Size([32, 19]), tgt shape: torch.Size([32, 15])\nTest batch - src shape: torch.Size([32, 14]), tgt shape: torch.Size([32, 13])\nTest batch - src shape: torch.Size([32, 13]), tgt shape: torch.Size([32, 11])\nTest batch - src shape: torch.Size([32, 17]), tgt shape: torch.Size([32, 16])\nTest batch - src shape: torch.Size([32, 19]), tgt shape: torch.Size([32, 20])\nTest batch - src shape: torch.Size([32, 13]), tgt shape: torch.Size([32, 12])\nTest batch - src shape: torch.Size([32, 14]), tgt shape: torch.Size([32, 14])\nTest batch - src shape: torch.Size([32, 10]), tgt shape: torch.Size([32, 11])\nTest batch - src shape: torch.Size([32, 15]), tgt shape: torch.Size([32, 15])\nTest batch - src shape: torch.Size([32, 19]), tgt shape: torch.Size([32, 20])\nTest batch - src shape: torch.Size([32, 14]), tgt shape: torch.Size([32, 15])\nTest batch - src shape: torch.Size([32, 16]), tgt shape: torch.Size([32, 15])\nTest batch - src shape: torch.Size([32, 9]), tgt shape: torch.Size([32, 8])\nTest batch - src shape: torch.Size([32, 15]), tgt shape: torch.Size([32, 13])\nTest batch - src shape: torch.Size([32, 11]), tgt shape: torch.Size([32, 10])\nTest batch - src shape: torch.Size([32, 12]), tgt shape: torch.Size([32, 10])\nTest batch - src shape: torch.Size([32, 15]), tgt shape: torch.Size([32, 13])\nTest batch - src shape: torch.Size([32, 17]), tgt shape: torch.Size([32, 15])\nTest batch - src shape: torch.Size([32, 16]), tgt shape: torch.Size([32, 14])\nTest batch - src shape: torch.Size([32, 14]), tgt shape: torch.Size([32, 14])\nTest batch - src shape: torch.Size([32, 19]), tgt shape: torch.Size([32, 13])\nTest batch - src shape: torch.Size([32, 20]), tgt shape: torch.Size([32, 18])\nTest batch - src shape: torch.Size([32, 20]), tgt shape: torch.Size([32, 20])\nTest batch - src shape: torch.Size([32, 17]), tgt shape: torch.Size([32, 13])\nTest batch - src shape: torch.Size([32, 12]), tgt shape: torch.Size([32, 12])\nTest batch - src shape: torch.Size([32, 20]), tgt shape: torch.Size([32, 18])\nTest batch - src shape: torch.Size([32, 16]), tgt shape: torch.Size([32, 16])\nTest batch - src shape: torch.Size([32, 15]), tgt shape: torch.Size([32, 13])\nTest batch - src shape: torch.Size([32, 14]), tgt shape: torch.Size([32, 13])\nTest batch - src shape: torch.Size([32, 14]), tgt shape: torch.Size([32, 10])\nTest batch - src shape: torch.Size([32, 16]), tgt shape: torch.Size([32, 13])\nTest batch - src shape: torch.Size([32, 20]), tgt shape: torch.Size([32, 18])\nTest batch - src shape: torch.Size([32, 20]), tgt shape: torch.Size([32, 18])\nTest batch - src shape: torch.Size([32, 20]), tgt shape: torch.Size([32, 17])\nTest batch - src shape: torch.Size([32, 19]), tgt shape: torch.Size([32, 18])\nTest batch - src shape: torch.Size([32, 22]), tgt shape: torch.Size([32, 19])\nTest batch - src shape: torch.Size([32, 18]), tgt shape: torch.Size([32, 16])\nTest batch - src shape: torch.Size([32, 16]), tgt shape: torch.Size([32, 17])\nTest batch - src shape: torch.Size([32, 20]), tgt shape: torch.Size([32, 19])\nTest batch - src shape: torch.Size([32, 19]), tgt shape: torch.Size([32, 20])\nTest batch - src shape: torch.Size([32, 22]), tgt shape: torch.Size([32, 14])\nTest batch - src shape: torch.Size([32, 14]), tgt shape: torch.Size([32, 14])\nTest batch - src shape: torch.Size([32, 14]), tgt shape: torch.Size([32, 13])\nTest batch - src shape: torch.Size([32, 16]), tgt shape: torch.Size([32, 15])\nTest batch - src shape: torch.Size([32, 17]), tgt shape: torch.Size([32, 14])\nTest batch - src shape: torch.Size([32, 13]), tgt shape: torch.Size([32, 10])\nTest batch - src shape: torch.Size([32, 9]), tgt shape: torch.Size([32, 10])\nTest batch - src shape: torch.Size([32, 11]), tgt shape: torch.Size([32, 10])\nTest batch - src shape: torch.Size([32, 9]), tgt shape: torch.Size([32, 10])\nTest batch - src shape: torch.Size([32, 10]), tgt shape: torch.Size([32, 11])\nTest batch - src shape: torch.Size([32, 13]), tgt shape: torch.Size([32, 13])\nTest batch - src shape: torch.Size([32, 16]), tgt shape: torch.Size([32, 14])\nTest batch - src shape: torch.Size([32, 21]), tgt shape: torch.Size([32, 18])\nTest batch - src shape: torch.Size([32, 21]), tgt shape: torch.Size([32, 17])\nTest batch - src shape: torch.Size([32, 25]), tgt shape: torch.Size([32, 19])\nTest batch - src shape: torch.Size([32, 20]), tgt shape: torch.Size([32, 15])\nTest batch - src shape: torch.Size([32, 20]), tgt shape: torch.Size([32, 13])\nTest batch - src shape: torch.Size([32, 20]), tgt shape: torch.Size([32, 17])\nTest batch - src shape: torch.Size([32, 17]), tgt shape: torch.Size([32, 13])\nTest batch - src shape: torch.Size([32, 20]), tgt shape: torch.Size([32, 15])\nTest batch - src shape: torch.Size([32, 17]), tgt shape: torch.Size([32, 14])\nTest batch - src shape: torch.Size([32, 21]), tgt shape: torch.Size([32, 19])\nTest batch - src shape: torch.Size([32, 24]), tgt shape: torch.Size([32, 23])\nTest batch - src shape: torch.Size([32, 21]), tgt shape: torch.Size([32, 19])\nTest batch - src shape: torch.Size([32, 16]), tgt shape: torch.Size([32, 15])\nTest batch - src shape: torch.Size([32, 17]), tgt shape: torch.Size([32, 16])\nTest batch - src shape: torch.Size([32, 17]), tgt shape: torch.Size([32, 18])\nTest batch - src shape: torch.Size([32, 16]), tgt shape: torch.Size([32, 14])\nTest batch - src shape: torch.Size([32, 18]), tgt shape: torch.Size([32, 16])\nTest batch - src shape: torch.Size([32, 19]), tgt shape: torch.Size([32, 16])\nTest batch - src shape: torch.Size([32, 20]), tgt shape: torch.Size([32, 16])\nTest batch - src shape: torch.Size([32, 19]), tgt shape: torch.Size([32, 15])\nTest batch - src shape: torch.Size([32, 14]), tgt shape: torch.Size([32, 13])\nTest batch - src shape: torch.Size([32, 19]), tgt shape: torch.Size([32, 15])\nTest batch - src shape: torch.Size([32, 22]), tgt shape: torch.Size([32, 17])\nTest batch - src shape: torch.Size([32, 19]), tgt shape: torch.Size([32, 17])\nTest batch - src shape: torch.Size([32, 15]), tgt shape: torch.Size([32, 15])\nTest batch - src shape: torch.Size([32, 19]), tgt shape: torch.Size([32, 19])\nTest batch - src shape: torch.Size([32, 17]), tgt shape: torch.Size([32, 15])\nTest batch - src shape: torch.Size([32, 19]), tgt shape: torch.Size([32, 14])\nTest batch - src shape: torch.Size([32, 21]), tgt shape: torch.Size([32, 17])\nTest batch - src shape: torch.Size([32, 14]), tgt shape: torch.Size([32, 14])\nTest batch - src shape: torch.Size([32, 21]), tgt shape: torch.Size([32, 17])\nTest batch - src shape: torch.Size([32, 17]), tgt shape: torch.Size([32, 17])\nTest batch - src shape: torch.Size([32, 19]), tgt shape: torch.Size([32, 16])\nTest batch - src shape: torch.Size([32, 19]), tgt shape: torch.Size([32, 12])\nTest batch - src shape: torch.Size([32, 16]), tgt shape: torch.Size([32, 15])\nTest batch - src shape: torch.Size([32, 20]), tgt shape: torch.Size([32, 19])\nTest batch - src shape: torch.Size([32, 22]), tgt shape: torch.Size([32, 18])\nTest batch - src shape: torch.Size([32, 18]), tgt shape: torch.Size([32, 14])\nTest batch - src shape: torch.Size([32, 19]), tgt shape: torch.Size([32, 16])\nTest batch - src shape: torch.Size([32, 19]), tgt shape: torch.Size([32, 16])\nTest batch - src shape: torch.Size([32, 19]), tgt shape: torch.Size([32, 19])\nTest batch - src shape: torch.Size([32, 14]), tgt shape: torch.Size([32, 11])\nTest batch - src shape: torch.Size([32, 18]), tgt shape: torch.Size([32, 15])\nTest batch - src shape: torch.Size([32, 16]), tgt shape: torch.Size([32, 16])\nTest batch - src shape: torch.Size([32, 21]), tgt shape: torch.Size([32, 13])\nTest batch - src shape: torch.Size([32, 15]), tgt shape: torch.Size([32, 12])\nTest batch - src shape: torch.Size([32, 17]), tgt shape: torch.Size([32, 15])\nTest batch - src shape: torch.Size([32, 18]), tgt shape: torch.Size([32, 16])\nTest batch - src shape: torch.Size([32, 17]), tgt shape: torch.Size([32, 16])\nTest batch - src shape: torch.Size([32, 17]), tgt shape: torch.Size([32, 15])\nTest batch - src shape: torch.Size([32, 18]), tgt shape: torch.Size([32, 16])\nTest batch - src shape: torch.Size([32, 16]), tgt shape: torch.Size([32, 16])\nTest batch - src shape: torch.Size([32, 14]), tgt shape: torch.Size([32, 14])\nTest batch - src shape: torch.Size([32, 17]), tgt shape: torch.Size([32, 14])\nTest batch - src shape: torch.Size([32, 23]), tgt shape: torch.Size([32, 17])\nTest batch - src shape: torch.Size([32, 17]), tgt shape: torch.Size([32, 15])\nTest batch - src shape: torch.Size([32, 23]), tgt shape: torch.Size([32, 24])\nTest batch - src shape: torch.Size([32, 19]), tgt shape: torch.Size([32, 15])\nTest batch - src shape: torch.Size([32, 18]), tgt shape: torch.Size([32, 16])\nTest batch - src shape: torch.Size([32, 18]), tgt shape: torch.Size([32, 17])\nTest batch - src shape: torch.Size([32, 13]), tgt shape: torch.Size([32, 9])\nTest batch - src shape: torch.Size([32, 15]), tgt shape: torch.Size([32, 12])\nTest batch - src shape: torch.Size([32, 18]), tgt shape: torch.Size([32, 16])\nTest batch - src shape: torch.Size([32, 18]), tgt shape: torch.Size([32, 13])\nTest batch - src shape: torch.Size([32, 23]), tgt shape: torch.Size([32, 18])\nTest batch - src shape: torch.Size([32, 20]), tgt shape: torch.Size([32, 16])\nTest batch - src shape: torch.Size([32, 22]), tgt shape: torch.Size([32, 17])\nTest batch - src shape: torch.Size([32, 20]), tgt shape: torch.Size([32, 18])\nTest batch - src shape: torch.Size([32, 16]), tgt shape: torch.Size([32, 14])\nTest batch - src shape: torch.Size([32, 22]), tgt shape: torch.Size([32, 18])\nTest batch - src shape: torch.Size([32, 18]), tgt shape: torch.Size([32, 14])\nTest batch - src shape: torch.Size([32, 20]), tgt shape: torch.Size([32, 17])\nTest batch - src shape: torch.Size([32, 16]), tgt shape: torch.Size([32, 13])\nTest batch - src shape: torch.Size([32, 19]), tgt shape: torch.Size([32, 16])\nTest batch - src shape: torch.Size([32, 21]), tgt shape: torch.Size([32, 18])\nTest batch - src shape: torch.Size([32, 16]), tgt shape: torch.Size([32, 15])\nTest batch - src shape: torch.Size([32, 13]), tgt shape: torch.Size([32, 14])\nTest batch - src shape: torch.Size([32, 15]), tgt shape: torch.Size([32, 12])\nTest batch - src shape: torch.Size([32, 19]), tgt shape: torch.Size([32, 15])\nTest batch - src shape: torch.Size([32, 14]), tgt shape: torch.Size([32, 12])\nTest batch - src shape: torch.Size([32, 11]), tgt shape: torch.Size([32, 9])\nTest batch - src shape: torch.Size([32, 13]), tgt shape: torch.Size([32, 13])\nTest batch - src shape: torch.Size([32, 13]), tgt shape: torch.Size([32, 11])\nTest batch - src shape: torch.Size([32, 20]), tgt shape: torch.Size([32, 15])\nTest batch - src shape: torch.Size([32, 19]), tgt shape: torch.Size([32, 15])\nTest batch - src shape: torch.Size([32, 15]), tgt shape: torch.Size([32, 13])\nTest batch - src shape: torch.Size([32, 21]), tgt shape: torch.Size([32, 15])\nTest batch - src shape: torch.Size([32, 21]), tgt shape: torch.Size([32, 17])\nTest batch - src shape: torch.Size([32, 19]), tgt shape: torch.Size([32, 15])\nTest batch - src shape: torch.Size([32, 14]), tgt shape: torch.Size([32, 13])\nTest batch - src shape: torch.Size([32, 19]), tgt shape: torch.Size([32, 16])\nTest batch - src shape: torch.Size([32, 21]), tgt shape: torch.Size([32, 17])\nTest batch - src shape: torch.Size([32, 19]), tgt shape: torch.Size([32, 19])\nTest batch - src shape: torch.Size([32, 13]), tgt shape: torch.Size([32, 13])\nTest batch - src shape: torch.Size([32, 19]), tgt shape: torch.Size([32, 17])\nTest batch - src shape: torch.Size([32, 24]), tgt shape: torch.Size([32, 20])\nTest batch - src shape: torch.Size([32, 18]), tgt shape: torch.Size([32, 15])\nTest batch - src shape: torch.Size([32, 20]), tgt shape: torch.Size([32, 16])\nTest batch - src shape: torch.Size([32, 17]), tgt shape: torch.Size([32, 17])\nTest batch - src shape: torch.Size([32, 19]), tgt shape: torch.Size([32, 16])\nTest batch - src shape: torch.Size([32, 19]), tgt shape: torch.Size([32, 16])\nTest batch - src shape: torch.Size([32, 19]), tgt shape: torch.Size([32, 16])\nTest batch - src shape: torch.Size([16, 9]), tgt shape: torch.Size([16, 8])\nTest Accuracy: 0.4907\nTest predictions saved to 'test_predictions.csv'\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
